CLIR
========
[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64, i64) apple_aarch64
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64) apple_aarch64
    sig7 = (i64, i64, i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 apple_aarch64
    sig10 = (i64, i64, i64) -> i64 apple_aarch64
    sig11 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:21 sig0
    fn1 = colocated u0:19 sig1
    fn2 = u0:6 sig2
    fn3 = colocated u0:23 sig3
    fn4 = u0:7 sig4
    fn5 = colocated u0:25 sig5
    fn6 = u0:7 sig6
    fn7 = u0:12 sig7
    fn8 = colocated u0:35 sig8
    fn9 = u0:1 sig9
    fn10 = u0:3 sig10
    fn11 = u0:3 sig11

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 2
    v4 = ishl_imm v3, 1  ; v3 = 2
    v5 = iconst.i64 3
    v6 = iconst.i64 3
    v7 = func_addr.i64 fn0
    v8 = iadd_imm v7, 3
    v9 = func_addr.i64 fn1
    stack_store v0, ss0
    v10 = stack_addr.i64 ss0
    v11 = call fn2(v10, v2, v4, v5, v6, v8, v9)  ; v5 = 3, v6 = 3
    v12 = stack_load.i64 ss0
    v13 = load.i64 v11
    v14 = func_addr.i64 fn3
    v15 = iadd_imm v14, 3
    v16 = iconst.i64 2
    v17 = iconst.i64 0
    call fn4(v13, v15, v16, v17)  ; v16 = 2, v17 = 0
    v18 = func_addr.i64 fn5
    v19 = iadd_imm v18, 3
    v20 = iconst.i64 2
    v21 = iconst.i64 1
    call fn6(v13, v19, v20, v21)  ; v20 = 2, v21 = 1
    v22 = load.i64 v13
    v23 = iadd_imm v11, 1
    v24 = iadd_imm v12, -8
    store aligned v23, v24
    v25 = func_addr.i64 fn8
    v26 = iadd_imm v25, 3
    stack_store v24, ss0
    v27 = stack_addr.i64 ss0
    v28 = call fn9(v26, v27)
    v29 = stack_load.i64 ss0
    v30 = isub v12, v29
    v31 = load.i64 v22+8
    v32 = ishl_imm v31, 3
    v33 = iadd v32, v30
    v34 = ushr_imm v33, 3
    store v34, v22+8
    v35 = call fn10(v0, v29, v28)
    v36 = call fn7(v29, v22, v28, v13)
    v37 = call fn11(v0, v29, v36)
    v38 = load.i64 v36
    v39 = iadd_imm v36, 8
    v40 = sshr_imm v38, 1
    return v40
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = sshr_imm v2, 1
    v5 = sshr_imm v3, 1
    v6 = iadd v4, v5
    v7 = ishl_imm v6, 1
    v8 = iadd_imm v0, 8
    store v7, v8
    v9 = load.i64 v1+8
    v10 = ishl_imm v9, 3
    v11 = iadd v0, v10
    v12 = load.i64 v1
    return_call_indirect sig0, v12(v11, v1, v8)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 1
    v4 = ishl_imm v3, 1  ; v3 = 1
    v5 = iconst.i64 2
    v6 = call fn0(v5)  ; v5 = 2
    store aligned v4, v6
    store aligned v2, v6+8
    v7 = iadd_imm v6, 1
    v8 = iconst.i64 2
    v9 = call fn1(v8)  ; v8 = 2
    store aligned v2, v9
    store aligned v7, v9+8
    v10 = iadd_imm v9, 1
    v11 = iadd_imm v0, 0
    store v10, v11
    v12 = load.i64 v1+8
    v13 = ishl_imm v12, 3
    v14 = iadd v0, v13
    v15 = load.i64 v1
    return_call_indirect sig2, v15(v14, v1, v11)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64) -> i64 apple_aarch64
    sig3 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1
    fn2 = u0:0 sig2

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 1
    v5 = ishl_imm v4, 1  ; v4 = 1
    v6 = iconst.i64 0
    v7 = call fn0(v6)  ; v6 = 0
    v8 = iadd_imm v7, 1
    v9 = iconst.i64 2
    v10 = call fn1(v9)  ; v9 = 2
    store aligned v5, v10
    store aligned v8, v10+8
    v11 = iadd_imm v10, 1
    v12 = iconst.i64 2
    v13 = call fn2(v12)  ; v12 = 2
    store aligned v3, v13
    store aligned v11, v13+8
    v14 = iadd_imm v13, 1
    v15 = iadd_imm v0, 8
    store v14, v15
    v16 = load.i64 v1+8
    v17 = ishl_imm v16, 3
    v18 = iadd v0, v17
    v19 = load.i64 v1
    return_call_indirect sig3, v19(v18, v1, v15)
}


[main$__lambda_3__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    fn0 = %Memmove sig0
    fn1 = colocated u0:14 sig1
    fn2 = colocated u0:17 sig2
    fn3 = u0:4 sig3

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, -8
    store aligned v3, v4
    v5 = iadd_imm v4, 16
    v6 = isub v0, v4
    v7 = call fn0(v5, v4, v6)
    v8 = isub v0, v5
    v9 = load.i64 v1+8
    v10 = ishl_imm v9, 3
    v11 = iadd v10, v8
    v12 = ushr_imm v11, 3
    store v12, v1+8
    v13 = iconst.i64 1
    v14 = func_addr.i64 fn1
    v15 = func_addr.i64 fn2
    v16 = iconst.i64 1
    v17 = call fn3(v2, v16, v5, v1, v13, v14, v15)  ; v16 = 1, v13 = 1
    v18 = load.i64 v17
    v19 = load.i64 v17+8
    v20 = load.i64 v17+16
    return_call_indirect sig4, v18(v19, v20)
}


[main$__lambda_4__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[main$__lambda_5__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 2
    v4 = sshr_imm v2, 1
    v5 = imul v4, v3  ; v3 = 2
    v6 = ishl_imm v5, 1
    v7 = iconst.i64 0
    v8 = ishl_imm v7, 1  ; v7 = 0
    v9 = iconst.i64 3
    v10 = ishl_imm v9, 1  ; v9 = 3
    v11 = iconst.i64 2
    v12 = call fn0(v11)  ; v11 = 2
    store aligned v8, v12
    store aligned v10, v12+8
    v13 = iadd_imm v12, 1
    v14 = iconst.i64 2
    v15 = call fn1(v14)  ; v14 = 2
    store aligned v6, v15
    store aligned v13, v15+8
    v16 = iadd_imm v15, 1
    v17 = iadd_imm v0, 0
    store v16, v17
    v18 = load.i64 v1+8
    v19 = ishl_imm v18, 3
    v20 = iadd v0, v19
    v21 = load.i64 v1
    return_call_indirect sig2, v21(v20, v1, v17)
}


[main$__lambda_6__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = global_value.i64 gv0
    v4 = iadd_imm v3, 8
    v5 = iconst.i64 0
    v6 = func_addr.i64 fn0
    v7 = func_addr.i64 fn1
    v8 = iconst.i64 0
    v9 = call fn2(v2, v8, v0, v4, v5, v6, v7)  ; v8 = 0, v5 = 0
    v10 = load.i64 v9
    v11 = load.i64 v9+8
    v12 = load.i64 v9+16
    v13 = call_indirect sig3, v10(v11, v12)
    v14 = load.i64 v13
    v15 = iadd_imm v13, 8
    v16 = iconst.i64 1
    v17 = ishl_imm v16, 1  ; v16 = 1
    v18 = iadd_imm v0, 0
    store v17, v18
    v19 = load.i64 v1+8
    v20 = ishl_imm v19, 3
    v21 = iadd v0, v20
    v22 = load.i64 v1
    return_call_indirect sig4, v22(v21, v1, v18)
}


[main$__lambda_7__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    sig5 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig6 = (i64, i64) -> i64 tail
    sig7 = (i64, i64, i64, i64) apple_aarch64
    sig8 = (i64, i64, i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64) -> i64 apple_aarch64
    fn0 = %Memmove sig0
    fn1 = colocated u0:27 sig1
    fn2 = u0:0 sig2
    fn3 = colocated u0:29 sig3
    fn4 = colocated u0:19 sig4
    fn5 = u0:6 sig5
    fn6 = colocated u0:31 sig6
    fn7 = u0:7 sig7
    fn8 = u0:12 sig8
    fn9 = colocated u0:33 sig9
    fn10 = u0:1 sig10

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iadd_imm v0, 8
    v4 = isub v0, v0
    v5 = call fn0(v3, v0, v4)
    v6 = isub v0, v3
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v8, v6
    v10 = ushr_imm v9, 3
    store v10, v1+8
    v11 = iconst.i64 5
    v12 = ishl_imm v11, 1  ; v11 = 5
    v13 = func_addr.i64 fn1
    v14 = iconst.i64 1
    v15 = iadd_imm v13, 3
    v16 = iconst.i64 3
    v17 = call fn2(v16)  ; v16 = 3
    store aligned v15, v17
    store aligned v14, v17+8  ; v14 = 1
    store aligned v2, v17+16
    v18 = iadd_imm v17, 1
    v19 = iconst.i64 3
    v20 = func_addr.i64 fn3
    v21 = iadd_imm v20, 3
    v22 = func_addr.i64 fn4
    stack_store v3, ss0
    v23 = stack_addr.i64 ss0
    v24 = call fn5(v23, v1, v12, v18, v19, v21, v22)  ; v19 = 3
    v25 = stack_load.i64 ss0
    v26 = load.i64 v24
    v27 = func_addr.i64 fn6
    v28 = iadd_imm v27, 3
    v29 = iconst.i64 2
    v30 = iconst.i64 0
    call fn7(v26, v28, v29, v30)  ; v29 = 2, v30 = 0
    v31 = load.i64 v26
    v32 = iadd_imm v24, 1
    v33 = iadd_imm v25, -8
    store aligned v32, v33
    v34 = func_addr.i64 fn9
    v35 = iadd_imm v34, 3
    stack_store v33, ss0
    v36 = stack_addr.i64 ss0
    v37 = call fn10(v35, v36)
    v38 = stack_load.i64 ss0
    v39 = isub v25, v38
    v40 = load.i64 v31+8
    v41 = ishl_imm v40, 3
    v42 = iadd v41, v39
    v43 = ushr_imm v42, 3
    store v43, v31+8
    return_call fn8(v38, v31, v37, v26)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:20 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
