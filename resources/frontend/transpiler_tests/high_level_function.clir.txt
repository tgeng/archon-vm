CLIR
========
[add__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = sshr_imm v2, 1
    v5 = sshr_imm v3, 1
    v6 = iadd v4, v5
    v7 = ishl_imm v6, 1
    v8 = iadd_imm v0, 8
    store v7, v8
    v9 = load.i64 v1+8
    v10 = ishl_imm v9, 3
    v11 = iadd v0, v10
    v12 = load.i64 v1
    return_call_indirect sig0, v12(v11, v1, v8)
}


[fold__specialized]
function u0:0(i64, i64, i64, i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64) -> i64 tail
    fn0 = u0:1 sig0
    fn1 = colocated u0:22 sig2

block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64):
    v6 = icmp slt v5, v1
    v9 = uextend.i32 v6
    br_table v9, block4, [block3, block2]

block3:
    jump block1(v3, v0)

block2:
    v10 = iadd_imm.i64 v2, -1
    v11 = ishl_imm.i64 v5, 3
    v12 = iadd v10, v11
    v13 = load.i64 v12
    v14 = ishl_imm.i64 v3, 1
    v15 = iadd_imm.i64 v0, -8
    store aligned v13, v15
    v16 = iadd_imm v15, -8
    store aligned v14, v16
    v17 = global_value.i64 gv0
    v18 = iadd_imm v17, 8
    stack_store v16, ss0
    v19 = stack_addr.i64 ss0
    v20 = call fn0(v4, v19)
    v21 = stack_load.i64 ss0
    v22 = call_indirect sig1, v20(v21, v18)
    v23 = load.i64 v22
    v24 = iadd_imm v22, 8
    v25 = iconst.i64 1
    v26 = iadd.i64 v5, v25  ; v25 = 1
    v27 = sshr_imm v23, 1
    return_call fn1(v24, v1, v2, v27, v4, v26)

block4:
    trap unreachable

block1(v7: i64, v8: i64):
    return v7
}


[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:20 sig1
    fn2 = colocated u0:22 sig2

block0(v0: i64):
    v1 = iconst.i64 3
    v2 = iconst.i64 1
    v3 = ishl_imm v2, 1  ; v2 = 1
    v4 = iconst.i64 2
    v5 = ishl_imm v4, 1  ; v4 = 2
    v6 = iconst.i64 3
    v7 = ishl_imm v6, 1  ; v6 = 3
    v8 = iconst.i64 3
    v9 = call fn0(v8)  ; v8 = 3
    store aligned v3, v9
    store aligned v5, v9+8
    store aligned v7, v9+16
    v10 = iadd_imm v9, 1
    v11 = iconst.i64 10
    v12 = func_addr.i64 fn1
    v13 = iadd_imm v12, 3
    v14 = iconst.i64 0
    return_call fn2(v0, v1, v10, v11, v13, v14)  ; v1 = 3, v11 = 10, v14 = 0
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:23 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
