CLIR
========
[f__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = load.i64 v0+16
    v5 = sshr_imm v2, 1
    v6 = sshr_imm v3, 1
    v7 = iadd v5, v6
    v8 = sshr_imm v4, 1
    v9 = iadd v7, v8
    v10 = ishl_imm v9, 1
    v11 = iadd_imm v0, 16
    store v10, v11
    v12 = load.i64 v1+8
    v13 = ishl_imm v12, 3
    v14 = iadd v0, v13
    v15 = load.i64 v1
    return_call_indirect sig0, v15(v14, v1, v11)
}


[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64) -> i64 tail
    sig3 = () -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 apple_aarch64
    sig5 = (i64, i64) -> i64 tail
    sig6 = () -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 apple_aarch64
    sig8 = (i64, i64) -> i64 tail
    fn0 = colocated u0:22 sig0
    fn1 = u0:0 sig1
    fn2 = colocated u0:25 sig2
    fn3 = colocated u0:14 sig3
    fn4 = u0:1 sig4
    fn5 = colocated u0:14 sig6
    fn6 = u0:1 sig7

block0(v0: i64):
    v1 = iconst.i64 0
    v2 = iconst.i64 1
    v3 = iconst.i64 2
    v4 = iconst.i64 3
    v5 = func_addr.i64 fn0
    v6 = iconst.i64 3
    v7 = iadd_imm v5, 3
    v8 = ishl_imm v4, 1  ; v4 = 3
    v9 = ishl_imm v1, 1  ; v1 = 0
    v10 = ishl_imm v2, 1  ; v2 = 1
    v11 = iconst.i64 5
    v12 = call fn1(v11)  ; v11 = 5
    store aligned v7, v12
    store aligned v6, v12+8  ; v6 = 3
    store aligned v8, v12+16
    store aligned v9, v12+24
    store aligned v10, v12+32
    v13 = func_addr.i64 fn2
    v14 = call fn3()
    v15 = iadd_imm v12, 1
    stack_store v0, ss0
    v16 = stack_addr.i64 ss0
    v17 = call fn4(v15, v16)
    v18 = stack_load.i64 ss0
    v19 = call_indirect sig5, v17(v18, v14)
    v20 = load.i64 v19
    v21 = iadd_imm v19, 8
    v22 = call fn5()
    v23 = iadd_imm v13, 3
    stack_store v21, ss0
    v24 = stack_addr.i64 ss0
    v25 = call fn6(v23, v24)
    v26 = stack_load.i64 ss0
    v27 = call_indirect sig8, v25(v26, v22)
    v28 = load.i64 v27
    v29 = iadd_imm v27, 8
    v30 = sshr_imm v20, 1
    v31 = sshr_imm v28, 1
    v32 = iadd v30, v31
    return v32
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = iconst.i64 4
    v3 = iconst.i64 5
    v4 = iadd v2, v3  ; v2 = 4, v3 = 5
    v5 = ishl_imm v4, 1
    v6 = iadd_imm v0, -8
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig0, v10(v9, v1, v6)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:24 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
