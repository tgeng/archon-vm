CLIR
========
[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64, i64) apple_aarch64
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64) apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64, i64, i64) apple_aarch64
    sig9 = (i64, i64, i64, i64) -> i64 tail
    sig10 = (i64, i64) -> i64 tail
    sig11 = (i64, i64) -> i64 apple_aarch64
    sig12 = (i64, i64, i64) -> i64 apple_aarch64
    sig13 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:21 sig0
    fn1 = colocated u0:19 sig1
    fn2 = u0:6 sig2
    fn3 = colocated u0:23 sig3
    fn4 = u0:7 sig4
    fn5 = colocated u0:25 sig5
    fn6 = u0:7 sig6
    fn7 = colocated u0:27 sig7
    fn8 = u0:7 sig8
    fn9 = u0:12 sig9
    fn10 = colocated u0:29 sig10
    fn11 = u0:1 sig11
    fn12 = u0:3 sig12
    fn13 = u0:3 sig13

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 0
    v4 = ishl_imm v3, 1  ; v3 = 0
    v5 = iconst.i64 3
    v6 = iconst.i64 3
    v7 = func_addr.i64 fn0
    v8 = iadd_imm v7, 3
    v9 = func_addr.i64 fn1
    stack_store v0, ss0
    v10 = stack_addr.i64 ss0
    v11 = call fn2(v10, v2, v4, v5, v6, v8, v9)  ; v5 = 3, v6 = 3
    v12 = stack_load.i64 ss0
    v13 = load.i64 v11
    v14 = func_addr.i64 fn3
    v15 = iadd_imm v14, 3
    v16 = iconst.i64 2
    v17 = iconst.i64 0
    call fn4(v13, v15, v16, v17)  ; v16 = 2, v17 = 0
    v18 = func_addr.i64 fn5
    v19 = iadd_imm v18, 3
    v20 = iconst.i64 2
    v21 = iconst.i64 1
    call fn6(v13, v19, v20, v21)  ; v20 = 2, v21 = 1
    v22 = func_addr.i64 fn7
    v23 = iadd_imm v22, 3
    v24 = iconst.i64 2
    v25 = iconst.i64 2
    call fn8(v13, v23, v24, v25)  ; v24 = 2, v25 = 2
    v26 = load.i64 v13
    v27 = iadd_imm v11, 1
    v28 = iadd_imm v12, -8
    store aligned v27, v28
    v29 = func_addr.i64 fn10
    v30 = iadd_imm v29, 3
    stack_store v28, ss0
    v31 = stack_addr.i64 ss0
    v32 = call fn11(v30, v31)
    v33 = stack_load.i64 ss0
    v34 = isub v12, v33
    v35 = load.i64 v26+8
    v36 = ishl_imm v35, 3
    v37 = iadd v36, v34
    v38 = ushr_imm v37, 3
    store v38, v26+8
    v39 = call fn12(v0, v33, v32)
    v40 = call fn9(v33, v26, v32, v13)
    v41 = call fn13(v0, v33, v40)
    v42 = load.i64 v40
    v43 = iadd_imm v40, 8
    v44 = sshr_imm v42, 1
    return v44
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = sshr_imm v2, 1
    v5 = sshr_imm v3, 1
    v6 = iadd v4, v5
    v7 = ishl_imm v6, 1
    v8 = iconst.i64 1
    v9 = ishl_imm v8, 1  ; v8 = 1
    v10 = ishl_imm v6, 1
    v11 = iconst.i64 2
    v12 = call fn0(v11)  ; v11 = 2
    store aligned v9, v12
    store aligned v10, v12+8
    v13 = iadd_imm v12, 1
    v14 = iconst.i64 2
    v15 = call fn1(v14)  ; v14 = 2
    store aligned v7, v15
    store aligned v13, v15+8
    v16 = iadd_imm v15, 1
    v17 = iadd_imm v0, 8
    store v16, v17
    v18 = load.i64 v1+8
    v19 = ishl_imm v18, 3
    v20 = iadd v0, v19
    v21 = load.i64 v1
    return_call_indirect sig2, v21(v20, v1, v17)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = sshr_imm v2, 1
    v5 = sshr_imm v3, 1
    v6 = isub v4, v5
    v7 = ishl_imm v6, 1
    v8 = iconst.i64 1
    v9 = ishl_imm v8, 1  ; v8 = 1
    v10 = ishl_imm v6, 1
    v11 = iconst.i64 2
    v12 = call fn0(v11)  ; v11 = 2
    store aligned v9, v12
    store aligned v10, v12+8
    v13 = iadd_imm v12, 1
    v14 = iconst.i64 2
    v15 = call fn1(v14)  ; v14 = 2
    store aligned v7, v15
    store aligned v13, v15+8
    v16 = iadd_imm v15, 1
    v17 = iadd_imm v0, 8
    store v16, v17
    v18 = load.i64 v1+8
    v19 = ishl_imm v18, 3
    v20 = iadd v0, v19
    v21 = load.i64 v1
    return_call_indirect sig2, v21(v20, v1, v17)
}


[main$__lambda_3__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 2
    v4 = sshr_imm v2, 1
    v5 = imul v4, v3  ; v3 = 2
    v6 = ishl_imm v5, 1
    v7 = iconst.i64 1
    v8 = ishl_imm v7, 1  ; v7 = 1
    v9 = ishl_imm v5, 1
    v10 = iconst.i64 2
    v11 = call fn0(v10)  ; v10 = 2
    store aligned v8, v11
    store aligned v9, v11+8
    v12 = iadd_imm v11, 1
    v13 = iconst.i64 2
    v14 = call fn1(v13)  ; v13 = 2
    store aligned v6, v14
    store aligned v12, v14+8
    v15 = iadd_imm v14, 1
    v16 = iadd_imm v0, 0
    store v15, v16
    v17 = load.i64 v1+8
    v18 = ishl_imm v17, 3
    v19 = iadd v0, v18
    v20 = load.i64 v1
    return_call_indirect sig2, v20(v19, v1, v16)
}


[main$__lambda_4__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname0
    gv2 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64, i64, i64) -> i64 apple_aarch64
    sig13 = (i64, i64) -> i64 tail
    sig14 = (i64, i64) -> i64 tail
    sig15 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig16 = (i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:14 sig4
    fn4 = colocated u0:17 sig5
    fn5 = u0:4 sig6
    fn6 = colocated u0:14 sig8
    fn7 = colocated u0:17 sig9
    fn8 = u0:4 sig10
    fn9 = %Memmove sig12
    fn10 = colocated u0:14 sig13
    fn11 = colocated u0:17 sig14
    fn12 = u0:4 sig15

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = global_value.i64 gv0
    v4 = iadd_imm v3, 8
    v5 = iconst.i64 2
    v6 = ishl_imm v5, 1  ; v5 = 2
    v7 = iadd_imm v0, -8
    store aligned v6, v7
    v8 = iconst.i64 1
    v9 = func_addr.i64 fn0
    v10 = func_addr.i64 fn1
    v11 = iconst.i64 0
    v12 = call fn2(v2, v11, v7, v4, v8, v9, v10)  ; v11 = 0, v8 = 1
    v13 = load.i64 v12
    v14 = load.i64 v12+8
    v15 = load.i64 v12+16
    v16 = call_indirect sig3, v13(v14, v15)
    v17 = load.i64 v16
    v18 = iadd_imm v16, 8
    v19 = global_value.i64 gv1
    v20 = iadd_imm v19, 8
    v21 = iconst.i64 4
    v22 = ishl_imm v21, 1  ; v21 = 4
    v23 = iadd_imm v18, -8
    store aligned v22, v23
    v24 = iconst.i64 1
    v25 = func_addr.i64 fn3
    v26 = func_addr.i64 fn4
    v27 = iconst.i64 0
    v28 = call fn5(v2, v27, v23, v20, v24, v25, v26)  ; v27 = 0, v24 = 1
    v29 = load.i64 v28
    v30 = load.i64 v28+8
    v31 = load.i64 v28+16
    v32 = call_indirect sig7, v29(v30, v31)
    v33 = load.i64 v32
    v34 = iadd_imm v32, 8
    v35 = global_value.i64 gv2
    v36 = iadd_imm v35, 8
    v37 = iconst.i64 1
    v38 = ishl_imm v37, 1  ; v37 = 1
    v39 = iadd_imm v34, -8
    store aligned v38, v39
    v40 = iconst.i64 1
    v41 = func_addr.i64 fn6
    v42 = func_addr.i64 fn7
    v43 = iconst.i64 1
    v44 = call fn8(v2, v43, v39, v36, v40, v41, v42)  ; v43 = 1, v40 = 1
    v45 = load.i64 v44
    v46 = load.i64 v44+8
    v47 = load.i64 v44+16
    v48 = call_indirect sig11, v45(v46, v47)
    v49 = load.i64 v48
    v50 = iadd_imm v48, 8
    v51 = iadd_imm v50, 8
    v52 = isub v0, v50
    v53 = call fn9(v51, v50, v52)
    v54 = isub v0, v51
    v55 = load.i64 v1+8
    v56 = ishl_imm v55, 3
    v57 = iadd v56, v54
    v58 = ushr_imm v57, 3
    store v58, v1+8
    v59 = iconst.i64 0
    v60 = func_addr.i64 fn10
    v61 = func_addr.i64 fn11
    v62 = iconst.i64 2
    v63 = call fn12(v2, v62, v51, v1, v59, v60, v61)  ; v62 = 2, v59 = 0
    v64 = load.i64 v63
    v65 = load.i64 v63+8
    v66 = load.i64 v63+16
    return_call_indirect sig16, v64(v65, v66)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:20 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
