CLIR
========
[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64, i64) apple_aarch64
    sig5 = (i64, i64, i64, i64) -> i64 tail
    sig6 = (i64, i64) -> i64 tail
    sig7 = (i64, i64) -> i64 apple_aarch64
    sig8 = (i64, i64, i64) -> i64 apple_aarch64
    sig9 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:21 sig0
    fn1 = colocated u0:19 sig1
    fn2 = u0:6 sig2
    fn3 = colocated u0:23 sig3
    fn4 = u0:7 sig4
    fn5 = u0:12 sig5
    fn6 = colocated u0:31 sig6
    fn7 = u0:1 sig7
    fn8 = u0:3 sig8
    fn9 = u0:3 sig9

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 1
    v4 = ishl_imm v3, 1  ; v3 = 1
    v5 = iconst.i64 3
    v6 = iconst.i64 3
    v7 = func_addr.i64 fn0
    v8 = iadd_imm v7, 3
    v9 = func_addr.i64 fn1
    stack_store v0, ss0
    v10 = stack_addr.i64 ss0
    v11 = call fn2(v10, v2, v4, v5, v6, v8, v9)  ; v5 = 3, v6 = 3
    v12 = stack_load.i64 ss0
    v13 = load.i64 v11
    v14 = func_addr.i64 fn3
    v15 = iadd_imm v14, 3
    v16 = iconst.i64 2
    v17 = iconst.i64 0
    call fn4(v13, v15, v16, v17)  ; v16 = 2, v17 = 0
    v18 = load.i64 v13
    v19 = iadd_imm v11, 1
    v20 = iadd_imm v12, -8
    store aligned v19, v20
    v21 = func_addr.i64 fn6
    v22 = iadd_imm v21, 3
    stack_store v20, ss0
    v23 = stack_addr.i64 ss0
    v24 = call fn7(v22, v23)
    v25 = stack_load.i64 ss0
    v26 = isub v12, v25
    v27 = load.i64 v18+8
    v28 = ishl_imm v27, 3
    v29 = iadd v28, v26
    v30 = ushr_imm v29, 3
    store v30, v18+8
    v31 = call fn8(v0, v25, v24)
    v32 = call fn5(v25, v18, v24, v13)
    v33 = call fn9(v0, v25, v32)
    v34 = load.i64 v32
    v35 = iadd_imm v32, 8
    v36 = sshr_imm v34, 1
    return v36
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 100
    v5 = sshr_imm v2, 1
    v6 = imul v5, v4  ; v4 = 100
    v7 = sshr_imm v3, 1
    v8 = iadd v6, v7
    v9 = ishl_imm v8, 1
    v10 = iadd_imm v0, 8
    store v9, v10
    v11 = load.i64 v1+8
    v12 = ishl_imm v11, 3
    v13 = iadd v0, v12
    v14 = load.i64 v1
    return_call_indirect sig0, v14(v13, v1, v10)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64) -> i64 apple_aarch64
    sig3 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1
    fn2 = u0:0 sig2

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = sshr_imm v2, 1
    v5 = sshr_imm v3, 1
    v6 = imul v4, v5
    v7 = ishl_imm v6, 1
    v8 = iconst.i64 1
    v9 = ishl_imm v8, 1  ; v8 = 1
    v10 = iconst.i64 0
    v11 = call fn0(v10)  ; v10 = 0
    v12 = iadd_imm v11, 1
    v13 = iconst.i64 2
    v14 = call fn1(v13)  ; v13 = 2
    store aligned v9, v14
    store aligned v12, v14+8
    v15 = iadd_imm v14, 1
    v16 = iconst.i64 2
    v17 = call fn2(v16)  ; v16 = 2
    store aligned v7, v17
    store aligned v15, v17+8
    v18 = iadd_imm v17, 1
    v19 = iadd_imm v0, 8
    store v18, v19
    v20 = load.i64 v1+8
    v21 = ishl_imm v20, 3
    v22 = iadd v0, v21
    v23 = load.i64 v1
    return_call_indirect sig3, v23(v22, v1, v19)
}


[main$__lambda_10__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = global_value.i64 gv0
    v4 = iadd_imm v3, 8
    v5 = iconst.i64 1
    v6 = ishl_imm v5, 1  ; v5 = 1
    v7 = iadd_imm v0, -8
    store aligned v6, v7
    v8 = iconst.i64 1
    v9 = func_addr.i64 fn0
    v10 = func_addr.i64 fn1
    v11 = iconst.i64 0
    v12 = call fn2(v2, v11, v7, v4, v8, v9, v10)  ; v11 = 0, v8 = 1
    v13 = load.i64 v12
    v14 = load.i64 v12+8
    v15 = load.i64 v12+16
    v16 = call_indirect sig3, v13(v14, v15)
    v17 = load.i64 v16
    v18 = iadd_imm v16, 8
    v19 = iconst.i64 0
    v20 = ishl_imm v19, 1  ; v19 = 0
    v21 = iadd_imm v0, 0
    store v20, v21
    v22 = load.i64 v1+8
    v23 = ishl_imm v22, 3
    v24 = iadd v0, v23
    v25 = load.i64 v1
    return_call_indirect sig4, v25(v24, v1, v21)
}


[main$__lambda_11__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64, i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64) -> i64 apple_aarch64
    sig10 = (i64, i64) -> i64 apple_aarch64
    fn0 = %Memmove sig0
    fn1 = u0:0 sig1
    fn2 = colocated u0:43 sig2
    fn3 = u0:0 sig3
    fn4 = colocated u0:35 sig4
    fn5 = colocated u0:19 sig5
    fn6 = u0:6 sig6
    fn7 = u0:12 sig7
    fn8 = colocated u0:25 sig8
    fn9 = u0:0 sig9
    fn10 = u0:1 sig10

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = load.i64 v0+16
    v5 = iadd_imm v0, 24
    v6 = isub v0, v0
    v7 = call fn0(v5, v0, v6)
    v8 = isub v0, v5
    v9 = load.i64 v1+8
    v10 = ishl_imm v9, 3
    v11 = iadd v10, v8
    v12 = ushr_imm v11, 3
    store v12, v1+8
    v13 = iconst.i64 0
    v14 = call fn1(v13)  ; v13 = 0
    v15 = iadd_imm v14, 1
    v16 = func_addr.i64 fn2
    v17 = iconst.i64 2
    v18 = iadd_imm v16, 3
    v19 = iconst.i64 4
    v20 = call fn3(v19)  ; v19 = 4
    store aligned v18, v20
    store aligned v17, v20+8  ; v17 = 2
    store aligned v2, v20+16
    store aligned v4, v20+24
    v21 = iadd_imm v20, 1
    v22 = iconst.i64 3
    v23 = func_addr.i64 fn4
    v24 = iadd_imm v23, 3
    v25 = func_addr.i64 fn5
    stack_store v5, ss0
    v26 = stack_addr.i64 ss0
    v27 = call fn6(v26, v1, v15, v21, v22, v24, v25)  ; v22 = 3
    v28 = stack_load.i64 ss0
    v29 = load.i64 v27
    v30 = load.i64 v29
    v31 = iadd_imm v27, 1
    v32 = iadd_imm v28, -8
    store aligned v31, v32
    v33 = func_addr.i64 fn8
    v34 = iconst.i64 1
    v35 = iadd_imm v33, 3
    v36 = iconst.i64 3
    v37 = call fn9(v36)  ; v36 = 3
    store aligned v35, v37
    store aligned v34, v37+8  ; v34 = 1
    store aligned v3, v37+16
    v38 = iadd_imm v37, 1
    stack_store v32, ss0
    v39 = stack_addr.i64 ss0
    v40 = call fn10(v38, v39)
    v41 = stack_load.i64 ss0
    v42 = isub v28, v41
    v43 = load.i64 v30+8
    v44 = ishl_imm v43, 3
    v45 = iadd v44, v42
    v46 = ushr_imm v45, 3
    store v46, v30+8
    return_call fn7(v41, v30, v40, v29)
}


[main$__lambda_12__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64) -> i64 apple_aarch64
    sig9 = (i64, i64, i64, i64) apple_aarch64
    sig10 = (i64, i64, i64, i64) -> i64 tail
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64) -> i64 apple_aarch64
    sig13 = (i64, i64) -> i64 apple_aarch64
    fn0 = %Memmove sig0
    fn1 = u0:0 sig1
    fn2 = colocated u0:39 sig2
    fn3 = u0:0 sig3
    fn4 = colocated u0:35 sig4
    fn5 = colocated u0:19 sig5
    fn6 = u0:6 sig6
    fn7 = colocated u0:41 sig7
    fn8 = u0:0 sig8
    fn9 = u0:7 sig9
    fn10 = u0:12 sig10
    fn11 = colocated u0:27 sig11
    fn12 = u0:0 sig12
    fn13 = u0:1 sig13

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 16
    v5 = isub v0, v0
    v6 = call fn0(v4, v0, v5)
    v7 = isub v0, v4
    v8 = load.i64 v1+8
    v9 = ishl_imm v8, 3
    v10 = iadd v9, v7
    v11 = ushr_imm v10, 3
    store v11, v1+8
    v12 = iconst.i64 0
    v13 = call fn1(v12)  ; v12 = 0
    v14 = iadd_imm v13, 1
    v15 = func_addr.i64 fn2
    v16 = iconst.i64 1
    v17 = iadd_imm v15, 3
    v18 = iconst.i64 3
    v19 = call fn3(v18)  ; v18 = 3
    store aligned v17, v19
    store aligned v16, v19+8  ; v16 = 1
    store aligned v2, v19+16
    v20 = iadd_imm v19, 1
    v21 = iconst.i64 3
    v22 = func_addr.i64 fn4
    v23 = iadd_imm v22, 3
    v24 = func_addr.i64 fn5
    stack_store v4, ss0
    v25 = stack_addr.i64 ss0
    v26 = call fn6(v25, v1, v14, v20, v21, v23, v24)  ; v21 = 3
    v27 = stack_load.i64 ss0
    v28 = load.i64 v26
    v29 = func_addr.i64 fn7
    v30 = iconst.i64 1
    v31 = iadd_imm v29, 3
    v32 = iconst.i64 3
    v33 = call fn8(v32)  ; v32 = 3
    store aligned v31, v33
    store aligned v30, v33+8  ; v30 = 1
    store aligned v2, v33+16
    v34 = iadd_imm v33, 1
    v35 = iconst.i64 2
    v36 = iconst.i64 0
    call fn9(v28, v34, v35, v36)  ; v35 = 2, v36 = 0
    v37 = load.i64 v28
    v38 = iadd_imm v26, 1
    v39 = iadd_imm v27, -8
    store aligned v38, v39
    v40 = func_addr.i64 fn11
    v41 = iconst.i64 2
    v42 = iadd_imm v40, 3
    v43 = iconst.i64 4
    v44 = call fn12(v43)  ; v43 = 4
    store aligned v42, v44
    store aligned v41, v44+8  ; v41 = 2
    store aligned v2, v44+16
    store aligned v3, v44+24
    v45 = iadd_imm v44, 1
    stack_store v39, ss0
    v46 = stack_addr.i64 ss0
    v47 = call fn13(v45, v46)
    v48 = stack_load.i64 ss0
    v49 = isub v27, v48
    v50 = load.i64 v37+8
    v51 = ishl_imm v50, 3
    v52 = iadd v51, v49
    v53 = ushr_imm v52, 3
    store v53, v37+8
    return_call fn10(v48, v37, v47, v28)
}


[main$__lambda_13__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64) -> i64 apple_aarch64
    sig9 = (i64, i64, i64, i64) apple_aarch64
    sig10 = (i64, i64, i64, i64) -> i64 tail
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64) -> i64 apple_aarch64
    sig13 = (i64, i64) -> i64 apple_aarch64
    fn0 = %Memmove sig0
    fn1 = u0:0 sig1
    fn2 = colocated u0:33 sig2
    fn3 = u0:0 sig3
    fn4 = colocated u0:35 sig4
    fn5 = colocated u0:19 sig5
    fn6 = u0:6 sig6
    fn7 = colocated u0:37 sig7
    fn8 = u0:0 sig8
    fn9 = u0:7 sig9
    fn10 = u0:12 sig10
    fn11 = colocated u0:29 sig11
    fn12 = u0:0 sig12
    fn13 = u0:1 sig13

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iadd_imm v0, 8
    v4 = isub v0, v0
    v5 = call fn0(v3, v0, v4)
    v6 = isub v0, v3
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v8, v6
    v10 = ushr_imm v9, 3
    store v10, v1+8
    v11 = iconst.i64 0
    v12 = call fn1(v11)  ; v11 = 0
    v13 = iadd_imm v12, 1
    v14 = func_addr.i64 fn2
    v15 = iconst.i64 1
    v16 = iadd_imm v14, 3
    v17 = iconst.i64 3
    v18 = call fn3(v17)  ; v17 = 3
    store aligned v16, v18
    store aligned v15, v18+8  ; v15 = 1
    store aligned v2, v18+16
    v19 = iadd_imm v18, 1
    v20 = iconst.i64 3
    v21 = func_addr.i64 fn4
    v22 = iadd_imm v21, 3
    v23 = func_addr.i64 fn5
    stack_store v3, ss0
    v24 = stack_addr.i64 ss0
    v25 = call fn6(v24, v1, v13, v19, v20, v22, v23)  ; v20 = 3
    v26 = stack_load.i64 ss0
    v27 = load.i64 v25
    v28 = func_addr.i64 fn7
    v29 = iconst.i64 1
    v30 = iadd_imm v28, 3
    v31 = iconst.i64 3
    v32 = call fn8(v31)  ; v31 = 3
    store aligned v30, v32
    store aligned v29, v32+8  ; v29 = 1
    store aligned v2, v32+16
    v33 = iadd_imm v32, 1
    v34 = iconst.i64 2
    v35 = iconst.i64 0
    call fn9(v27, v33, v34, v35)  ; v34 = 2, v35 = 0
    v36 = load.i64 v27
    v37 = iadd_imm v25, 1
    v38 = iadd_imm v26, -8
    store aligned v37, v38
    v39 = func_addr.i64 fn11
    v40 = iconst.i64 1
    v41 = iadd_imm v39, 3
    v42 = iconst.i64 3
    v43 = call fn12(v42)  ; v42 = 3
    store aligned v41, v43
    store aligned v40, v43+8  ; v40 = 1
    store aligned v2, v43+16
    v44 = iadd_imm v43, 1
    stack_store v38, ss0
    v45 = stack_addr.i64 ss0
    v46 = call fn13(v44, v45)
    v47 = stack_load.i64 ss0
    v48 = isub v26, v47
    v49 = load.i64 v36+8
    v50 = ishl_imm v49, 3
    v51 = iadd v50, v48
    v52 = ushr_imm v51, 3
    store v52, v36+8
    return_call fn10(v47, v36, v46, v27)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    fn0 = %Memmove sig0
    fn1 = colocated u0:14 sig1
    fn2 = colocated u0:17 sig2
    fn3 = u0:4 sig3

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 11
    v5 = ishl_imm v4, 1  ; v4 = 11
    v6 = iadd_imm v0, -8
    store aligned v5, v6
    v7 = iadd_imm v6, 16
    v8 = isub v0, v6
    v9 = call fn0(v7, v6, v8)
    v10 = isub v0, v7
    v11 = load.i64 v1+8
    v12 = ishl_imm v11, 3
    v13 = iadd v12, v10
    v14 = ushr_imm v13, 3
    store v14, v1+8
    v15 = iconst.i64 1
    v16 = func_addr.i64 fn1
    v17 = func_addr.i64 fn2
    v18 = iconst.i64 0
    v19 = call fn3(v2, v18, v7, v1, v15, v16, v17)  ; v18 = 0, v15 = 1
    v20 = load.i64 v19
    v21 = load.i64 v19+8
    v22 = load.i64 v19+16
    return_call_indirect sig4, v20(v21, v22)
}


[main$__lambda_3__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[main$__lambda_4__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64) -> i64 apple_aarch64
    sig5 = (i64) -> i64 apple_aarch64
    sig6 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = u0:0 sig4
    fn4 = u0:0 sig5

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = load.i64 v0+16
    v5 = global_value.i64 gv0
    v6 = iadd_imm v5, 8
    v7 = iconst.i64 2
    v8 = ishl_imm v7, 1  ; v7 = 2
    v9 = iadd_imm v0, -8
    store aligned v8, v9
    v10 = iconst.i64 1
    v11 = func_addr.i64 fn0
    v12 = func_addr.i64 fn1
    v13 = iconst.i64 0
    v14 = call fn2(v2, v13, v9, v6, v10, v11, v12)  ; v13 = 0, v10 = 1
    v15 = load.i64 v14
    v16 = load.i64 v14+8
    v17 = load.i64 v14+16
    v18 = call_indirect sig3, v15(v16, v17)
    v19 = load.i64 v18
    v20 = iadd_imm v18, 8
    v21 = iconst.i64 0
    v22 = ishl_imm v21, 1  ; v21 = 0
    v23 = iconst.i64 2
    v24 = call fn3(v23)  ; v23 = 2
    store aligned v22, v24
    store aligned v4, v24+8
    v25 = iadd_imm v24, 1
    v26 = iconst.i64 2
    v27 = call fn4(v26)  ; v26 = 2
    store aligned v3, v27
    store aligned v25, v27+8
    v28 = iadd_imm v27, 1
    v29 = iadd_imm v0, 16
    store v28, v29
    v30 = load.i64 v1+8
    v31 = ishl_imm v30, 3
    v32 = iadd v0, v31
    v33 = load.i64 v1
    return_call_indirect sig6, v33(v32, v1, v29)
}


[main$__lambda_5__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    fn0 = %Memmove sig0
    fn1 = colocated u0:14 sig1
    fn2 = colocated u0:17 sig2
    fn3 = u0:4 sig3

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 7
    v5 = ishl_imm v4, 1  ; v4 = 7
    v6 = iadd_imm v0, -8
    store aligned v5, v6
    v7 = iadd_imm v6, 16
    v8 = isub v0, v6
    v9 = call fn0(v7, v6, v8)
    v10 = isub v0, v7
    v11 = load.i64 v1+8
    v12 = ishl_imm v11, 3
    v13 = iadd v12, v10
    v14 = ushr_imm v13, 3
    store v14, v1+8
    v15 = iconst.i64 1
    v16 = func_addr.i64 fn1
    v17 = func_addr.i64 fn2
    v18 = iconst.i64 0
    v19 = call fn3(v2, v18, v7, v1, v15, v16, v17)  ; v18 = 0, v15 = 1
    v20 = load.i64 v19
    v21 = load.i64 v19+8
    v22 = load.i64 v19+16
    return_call_indirect sig4, v20(v21, v22)
}


[main$__lambda_7__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64) -> i64 apple_aarch64
    sig5 = (i64) -> i64 apple_aarch64
    sig6 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = u0:0 sig4
    fn4 = u0:0 sig5

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = load.i64 v0+16
    v5 = global_value.i64 gv0
    v6 = iadd_imm v5, 8
    v7 = iconst.i64 3
    v8 = ishl_imm v7, 1  ; v7 = 3
    v9 = iadd_imm v0, -8
    store aligned v8, v9
    v10 = iconst.i64 1
    v11 = func_addr.i64 fn0
    v12 = func_addr.i64 fn1
    v13 = iconst.i64 0
    v14 = call fn2(v2, v13, v9, v6, v10, v11, v12)  ; v13 = 0, v10 = 1
    v15 = load.i64 v14
    v16 = load.i64 v14+8
    v17 = load.i64 v14+16
    v18 = call_indirect sig3, v15(v16, v17)
    v19 = load.i64 v18
    v20 = iadd_imm v18, 8
    v21 = iconst.i64 0
    v22 = ishl_imm v21, 1  ; v21 = 0
    v23 = iconst.i64 2
    v24 = call fn3(v23)  ; v23 = 2
    store aligned v22, v24
    store aligned v4, v24+8
    v25 = iadd_imm v24, 1
    v26 = iconst.i64 2
    v27 = call fn4(v26)  ; v26 = 2
    store aligned v3, v27
    store aligned v25, v27+8
    v28 = iadd_imm v27, 1
    v29 = iadd_imm v0, 16
    store v28, v29
    v30 = load.i64 v1+8
    v31 = ishl_imm v30, 3
    v32 = iadd v0, v31
    v33 = load.i64 v1
    return_call_indirect sig6, v33(v32, v1, v29)
}


[main$__lambda_8__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 apple_aarch64
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64) -> i64 tail
    sig7 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig8 = (i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = %Memmove sig4
    fn4 = colocated u0:14 sig5
    fn5 = colocated u0:17 sig6
    fn6 = u0:4 sig7

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = load.i64 v0+16
    v5 = global_value.i64 gv0
    v6 = iadd_imm v5, 8
    v7 = iconst.i64 5
    v8 = ishl_imm v7, 1  ; v7 = 5
    v9 = iadd_imm v0, -8
    store aligned v8, v9
    v10 = iconst.i64 1
    v11 = func_addr.i64 fn0
    v12 = func_addr.i64 fn1
    v13 = iconst.i64 0
    v14 = call fn2(v2, v13, v9, v6, v10, v11, v12)  ; v13 = 0, v10 = 1
    v15 = load.i64 v14
    v16 = load.i64 v14+8
    v17 = load.i64 v14+16
    v18 = call_indirect sig3, v15(v16, v17)
    v19 = load.i64 v18
    v20 = iadd_imm v18, 8
    v21 = iconst.i64 2
    v22 = ishl_imm v21, 1  ; v21 = 2
    v23 = iadd_imm v20, -8
    store aligned v22, v23
    v24 = iadd_imm v23, 24
    v25 = isub v0, v23
    v26 = call fn3(v24, v23, v25)
    v27 = isub v0, v24
    v28 = load.i64 v1+8
    v29 = ishl_imm v28, 3
    v30 = iadd v29, v27
    v31 = ushr_imm v30, 3
    store v31, v1+8
    v32 = iconst.i64 1
    v33 = func_addr.i64 fn4
    v34 = func_addr.i64 fn5
    v35 = iconst.i64 0
    v36 = call fn6(v3, v35, v24, v1, v32, v33, v34)  ; v35 = 0, v32 = 1
    v37 = load.i64 v36
    v38 = load.i64 v36+8
    v39 = load.i64 v36+16
    return_call_indirect sig8, v37(v38, v39)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:20 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
