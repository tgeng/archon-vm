CLIR
========
[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64, i64) apple_aarch64
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64) apple_aarch64
    sig7 = (i64, i64, i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 apple_aarch64
    sig10 = (i64, i64, i64) -> i64 apple_aarch64
    sig11 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:21 sig0
    fn1 = colocated u0:19 sig1
    fn2 = u0:6 sig2
    fn3 = colocated u0:23 sig3
    fn4 = u0:7 sig4
    fn5 = colocated u0:25 sig5
    fn6 = u0:7 sig6
    fn7 = u0:12 sig7
    fn8 = colocated u0:33 sig8
    fn9 = u0:1 sig9
    fn10 = u0:3 sig10
    fn11 = u0:3 sig11

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 0
    v4 = ishl_imm v3, 1  ; v3 = 0
    v5 = iconst.i64 3
    v6 = iconst.i64 3
    v7 = func_addr.i64 fn0
    v8 = iadd_imm v7, 3
    v9 = func_addr.i64 fn1
    stack_store v0, ss0
    v10 = stack_addr.i64 ss0
    v11 = call fn2(v10, v2, v4, v5, v6, v8, v9)  ; v5 = 3, v6 = 3
    v12 = stack_load.i64 ss0
    v13 = load.i64 v11
    v14 = func_addr.i64 fn3
    v15 = iadd_imm v14, 3
    v16 = iconst.i64 3
    v17 = iconst.i64 0
    call fn4(v13, v15, v16, v17)  ; v16 = 3, v17 = 0
    v18 = func_addr.i64 fn5
    v19 = iadd_imm v18, 3
    v20 = iconst.i64 3
    v21 = iconst.i64 1
    call fn6(v13, v19, v20, v21)  ; v20 = 3, v21 = 1
    v22 = load.i64 v13
    v23 = iadd_imm v11, 1
    v24 = iadd_imm v12, -8
    store aligned v23, v24
    v25 = func_addr.i64 fn8
    v26 = iadd_imm v25, 3
    stack_store v24, ss0
    v27 = stack_addr.i64 ss0
    v28 = call fn9(v26, v27)
    v29 = stack_load.i64 ss0
    v30 = isub v12, v29
    v31 = load.i64 v22+8
    v32 = ishl_imm v31, 3
    v33 = iadd v32, v30
    v34 = ushr_imm v33, 3
    store v34, v22+8
    v35 = call fn10(v0, v29, v28)
    v36 = call fn7(v29, v22, v28, v13)
    v37 = call fn11(v0, v29, v36)
    v38 = load.i64 v36
    v39 = iadd_imm v36, 8
    v40 = sshr_imm v38, 1
    return v40
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = u0:1 sig0
    fn1 = %Memmove sig2

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 0
    v5 = ishl_imm v4, 1  ; v4 = 0
    v6 = iadd_imm v0, -8
    store aligned v2, v6
    v7 = iadd_imm v6, -8
    store aligned v2, v7
    v8 = iadd_imm v7, -8
    store aligned v5, v8
    stack_store v8, ss0
    v9 = stack_addr.i64 ss0
    v10 = call fn0(v3, v9)
    v11 = stack_load.i64 ss0
    v12 = iadd_imm v11, 16
    v13 = isub v0, v11
    v14 = call fn1(v12, v11, v13)
    v15 = isub v0, v12
    v16 = load.i64 v1+8
    v17 = ishl_imm v16, 3
    v18 = iadd v17, v15
    v19 = ushr_imm v18, 3
    store v19, v1+8
    return_call_indirect sig1, v10(v12, v1)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 apple_aarch64
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = u0:0 sig0
    fn1 = u0:1 sig1
    fn2 = %Memmove sig3

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = load.i64 v0+16
    v5 = iconst.i64 0
    v6 = ishl_imm v5, 1  ; v5 = 0
    v7 = iconst.i64 0
    v8 = call fn0(v7)  ; v7 = 0
    v9 = iadd_imm v8, 1
    v10 = iadd_imm v0, -8
    store aligned v9, v10
    v11 = iadd_imm v10, -8
    store aligned v3, v11
    v12 = iadd_imm v11, -8
    store aligned v6, v12
    stack_store v12, ss0
    v13 = stack_addr.i64 ss0
    v14 = call fn1(v4, v13)
    v15 = stack_load.i64 ss0
    v16 = iadd_imm v15, 24
    v17 = isub v0, v15
    v18 = call fn2(v16, v15, v17)
    v19 = isub v0, v16
    v20 = load.i64 v1+8
    v21 = ishl_imm v20, 3
    v22 = iadd v21, v19
    v23 = ushr_imm v22, 3
    store v23, v1+8
    return_call_indirect sig2, v14(v16, v1)
}


[main$__lambda_4__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 apple_aarch64
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:14 sig4
    fn4 = colocated u0:17 sig5
    fn5 = u0:4 sig6
    fn6 = u0:1 sig8
    fn7 = %Memmove sig10

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block4

block4:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2, block3]

block1:
    v9 = load.i64 v0
    v10 = isub.i64 v0, v5
    v11 = ushr_imm v10, 3
    store v11, v1+8
    v12 = iconst.i64 1
    store v12, v1+24  ; v12 = 1
    v13 = iconst.i64 0
    v14 = func_addr.i64 fn0
    v15 = func_addr.i64 fn1
    v16 = iconst.i64 0
    v17 = call fn2(v9, v16, v5, v1, v13, v14, v15)  ; v16 = 0, v13 = 0
    v18 = load.i64 v17
    v19 = load.i64 v17+8
    v20 = load.i64 v17+16
    return_call_indirect sig3, v18(v19, v20)

block2:
    v21 = iconst.i64 1
    v22 = sshr_imm.i64 v6, 1
    v23 = iadd v22, v21  ; v21 = 1
    v24 = load.i64 v0
    v25 = ishl_imm v23, 1
    v26 = iadd_imm.i64 v5, -8
    store aligned v25, v26
    v27 = isub.i64 v0, v26
    v28 = ushr_imm v27, 3
    store v28, v1+8
    v29 = iconst.i64 2
    store v29, v1+24  ; v29 = 2
    store.i64 v6, v4
    v30 = ishl_imm v23, 1
    store v30, v4+8
    v31 = iconst.i64 1
    v32 = func_addr.i64 fn3
    v33 = func_addr.i64 fn4
    v34 = iconst.i64 1
    v35 = call fn5(v24, v34, v26, v1, v31, v32, v33)  ; v34 = 1, v31 = 1
    v36 = load.i64 v35
    v37 = load.i64 v35+8
    v38 = load.i64 v35+16
    return_call_indirect sig7, v36(v37, v38)

block3:
    v39 = iconst.i64 0
    v40 = ishl_imm v39, 1  ; v39 = 0
    v41 = load.i64 v0+8
    v42 = iadd_imm.i64 v5, -8
    store.i64 aligned v6, v42
    v43 = iadd_imm v42, -8
    store aligned v41, v43
    v44 = iadd_imm v43, -8
    store aligned v40, v44
    v45 = load.i64 v0+16
    stack_store v44, ss0
    v46 = stack_addr.i64 ss0
    v47 = call fn6(v45, v46)
    v48 = stack_load.i64 ss0
    v49 = load.i64 v1+16
    v50 = iadd_imm v48, 24
    v51 = isub.i64 v0, v48
    v52 = call fn7(v50, v48, v51)
    v53 = isub.i64 v0, v50
    v54 = load.i64 v49+8
    v55 = ishl_imm v54, 3
    v56 = iadd v55, v53
    v57 = ushr_imm v56, 3
    store v57, v49+8
    return_call_indirect sig9, v47(v50, v49)
}


[main$__lambda_4__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:28 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 56
    v3 = call fn0(v2)  ; v2 = 56
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[main$__lambda_5__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 apple_aarch64
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:14 sig4
    fn4 = colocated u0:17 sig5
    fn5 = u0:4 sig6
    fn6 = u0:1 sig8
    fn7 = %Memmove sig10

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block4

block4:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2, block3]

block1:
    v9 = load.i64 v0
    v10 = isub.i64 v0, v5
    v11 = ushr_imm v10, 3
    store v11, v1+8
    v12 = iconst.i64 1
    store v12, v1+24  ; v12 = 1
    v13 = iconst.i64 0
    v14 = func_addr.i64 fn0
    v15 = func_addr.i64 fn1
    v16 = iconst.i64 0
    v17 = call fn2(v9, v16, v5, v1, v13, v14, v15)  ; v16 = 0, v13 = 0
    v18 = load.i64 v17
    v19 = load.i64 v17+8
    v20 = load.i64 v17+16
    return_call_indirect sig3, v18(v19, v20)

block2:
    v21 = iconst.i64 2
    v22 = sshr_imm.i64 v6, 1
    v23 = imul v22, v21  ; v21 = 2
    v24 = load.i64 v0
    v25 = ishl_imm v23, 1
    v26 = iadd_imm.i64 v5, -8
    store aligned v25, v26
    v27 = isub.i64 v0, v26
    v28 = ushr_imm v27, 3
    store v28, v1+8
    v29 = iconst.i64 2
    store v29, v1+24  ; v29 = 2
    store.i64 v6, v4
    v30 = ishl_imm v23, 1
    store v30, v4+8
    v31 = iconst.i64 1
    v32 = func_addr.i64 fn3
    v33 = func_addr.i64 fn4
    v34 = iconst.i64 1
    v35 = call fn5(v24, v34, v26, v1, v31, v32, v33)  ; v34 = 1, v31 = 1
    v36 = load.i64 v35
    v37 = load.i64 v35+8
    v38 = load.i64 v35+16
    return_call_indirect sig7, v36(v37, v38)

block3:
    v39 = iconst.i64 0
    v40 = ishl_imm v39, 1  ; v39 = 0
    v41 = load.i64 v0+8
    v42 = iadd_imm.i64 v5, -8
    store.i64 aligned v6, v42
    v43 = iadd_imm v42, -8
    store aligned v41, v43
    v44 = iadd_imm v43, -8
    store aligned v40, v44
    v45 = load.i64 v0+16
    stack_store v44, ss0
    v46 = stack_addr.i64 ss0
    v47 = call fn6(v45, v46)
    v48 = stack_load.i64 ss0
    v49 = load.i64 v1+16
    v50 = iadd_imm v48, 24
    v51 = isub.i64 v0, v48
    v52 = call fn7(v50, v48, v51)
    v53 = isub.i64 v0, v50
    v54 = load.i64 v49+8
    v55 = ishl_imm v54, 3
    v56 = iadd v55, v53
    v57 = ushr_imm v56, 3
    store v57, v49+8
    return_call_indirect sig9, v47(v50, v49)
}


[main$__lambda_5__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:30 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 56
    v3 = call fn0(v2)  ; v2 = 56
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[main$__lambda_6__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64, i64) -> i64 tail
    sig13 = (i64, i64) -> i64 tail
    sig14 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig15 = (i64, i64) -> i64 tail
    sig16 = (i64, i64, i64) -> i64 apple_aarch64
    sig17 = (i64, i64) -> i64 tail
    sig18 = (i64, i64) -> i64 tail
    sig19 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig20 = (i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:14 sig4
    fn4 = colocated u0:17 sig5
    fn5 = u0:4 sig6
    fn6 = colocated u0:14 sig8
    fn7 = colocated u0:17 sig9
    fn8 = u0:4 sig10
    fn9 = colocated u0:14 sig12
    fn10 = colocated u0:17 sig13
    fn11 = u0:4 sig14
    fn12 = %Memmove sig16
    fn13 = colocated u0:14 sig17
    fn14 = colocated u0:17 sig18
    fn15 = u0:4 sig19

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block6

block6:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2, block3, block4, block5]

block1:
    v9 = load.i64 v0
    v10 = iconst.i64 1
    v11 = ishl_imm v10, 1  ; v10 = 1
    v12 = iadd_imm.i64 v5, -8
    store aligned v11, v12
    v13 = isub.i64 v0, v12
    v14 = ushr_imm v13, 3
    store v14, v1+8
    v15 = iconst.i64 1
    store v15, v1+24  ; v15 = 1
    v16 = iconst.i64 1
    v17 = func_addr.i64 fn0
    v18 = func_addr.i64 fn1
    v19 = iconst.i64 1
    v20 = call fn2(v9, v19, v12, v1, v16, v17, v18)  ; v19 = 1, v16 = 1
    v21 = load.i64 v20
    v22 = load.i64 v20+8
    v23 = load.i64 v20+16
    return_call_indirect sig3, v21(v22, v23)

block2:
    v24 = load.i64 v0+8
    v25 = isub.i64 v0, v5
    v26 = ushr_imm v25, 3
    store v26, v1+8
    v27 = iconst.i64 2
    store v27, v1+24  ; v27 = 2
    store.i64 v6, v4
    v28 = iconst.i64 0
    v29 = func_addr.i64 fn3
    v30 = func_addr.i64 fn4
    v31 = iconst.i64 0
    v32 = call fn5(v24, v31, v5, v1, v28, v29, v30)  ; v31 = 0, v28 = 0
    v33 = load.i64 v32
    v34 = load.i64 v32+8
    v35 = load.i64 v32+16
    return_call_indirect sig7, v33(v34, v35)

block3:
    v36 = load.i64 v0+8
    v37 = isub.i64 v0, v5
    v38 = ushr_imm v37, 3
    store v38, v1+8
    v39 = iconst.i64 3
    store v39, v1+24  ; v39 = 3
    store.i64 v6, v4+8
    v40 = iconst.i64 0
    v41 = func_addr.i64 fn6
    v42 = func_addr.i64 fn7
    v43 = iconst.i64 0
    v44 = call fn8(v36, v43, v5, v1, v40, v41, v42)  ; v43 = 0, v40 = 0
    v45 = load.i64 v44
    v46 = load.i64 v44+8
    v47 = load.i64 v44+16
    return_call_indirect sig11, v45(v46, v47)

block4:
    v48 = load.i64 v0+8
    v49 = isub.i64 v0, v5
    v50 = ushr_imm v49, 3
    store v50, v1+8
    v51 = iconst.i64 4
    store v51, v1+24  ; v51 = 4
    store.i64 v6, v4+16
    v52 = iconst.i64 0
    v53 = func_addr.i64 fn9
    v54 = func_addr.i64 fn10
    v55 = iconst.i64 1
    v56 = call fn11(v48, v55, v5, v1, v52, v53, v54)  ; v55 = 1, v52 = 0
    v57 = load.i64 v56
    v58 = load.i64 v56+8
    v59 = load.i64 v56+16
    return_call_indirect sig15, v57(v58, v59)

block5:
    v60 = load.i64 v0
    v61 = load.i64 v1+16
    v62 = iadd_imm.i64 v5, 16
    v63 = isub.i64 v0, v5
    v64 = call fn12(v62, v5, v63)
    v65 = isub.i64 v0, v62
    v66 = load.i64 v61+8
    v67 = ishl_imm v66, 3
    v68 = iadd v67, v65
    v69 = ushr_imm v68, 3
    store v69, v61+8
    v70 = iconst.i64 0
    v71 = func_addr.i64 fn13
    v72 = func_addr.i64 fn14
    v73 = iconst.i64 0
    v74 = call fn15(v60, v73, v62, v61, v70, v71, v72)  ; v73 = 0, v70 = 0
    v75 = load.i64 v74
    v76 = load.i64 v74+8
    v77 = load.i64 v74+16
    return_call_indirect sig20, v75(v76, v77)
}


[main$__lambda_6__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:32 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 64
    v3 = call fn0(v2)  ; v2 = 64
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[main$__lambda_7__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64) -> i64 tail
    sig3 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64) -> i64 apple_aarch64
    sig6 = (i64, i64, i64, i64) apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64) -> i64 apple_aarch64
    sig9 = (i64, i64, i64, i64) apple_aarch64
    sig10 = (i64, i64, i64, i64) -> i64 tail
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64) -> i64 apple_aarch64
    sig13 = (i64, i64) -> i64 apple_aarch64
    fn0 = %Memmove sig0
    fn1 = colocated u0:21 sig1
    fn2 = colocated u0:19 sig2
    fn3 = u0:6 sig3
    fn4 = colocated u0:27 sig4
    fn5 = u0:0 sig5
    fn6 = u0:7 sig6
    fn7 = colocated u0:29 sig7
    fn8 = u0:0 sig8
    fn9 = u0:7 sig9
    fn10 = u0:12 sig10
    fn11 = colocated u0:31 sig11
    fn12 = u0:0 sig12
    fn13 = u0:1 sig13

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iadd_imm v0, 8
    v4 = isub v0, v0
    v5 = call fn0(v3, v0, v4)
    v6 = isub v0, v3
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v8, v6
    v10 = ushr_imm v9, 3
    store v10, v1+8
    v11 = iconst.i64 1
    v12 = ishl_imm v11, 1  ; v11 = 1
    v13 = iconst.i64 3
    v14 = iconst.i64 3
    v15 = func_addr.i64 fn1
    v16 = iadd_imm v15, 3
    v17 = func_addr.i64 fn2
    stack_store v3, ss0
    v18 = stack_addr.i64 ss0
    v19 = call fn3(v18, v1, v12, v13, v14, v16, v17)  ; v13 = 3, v14 = 3
    v20 = stack_load.i64 ss0
    v21 = load.i64 v19
    v22 = func_addr.i64 fn4
    v23 = iconst.i64 1
    v24 = iadd_imm v22, 3
    v25 = iconst.i64 3
    v26 = call fn5(v25)  ; v25 = 3
    store aligned v24, v26
    store aligned v23, v26+8  ; v23 = 1
    store aligned v2, v26+16
    v27 = iadd_imm v26, 1
    v28 = iconst.i64 3
    v29 = iconst.i64 0
    call fn6(v21, v27, v28, v29)  ; v28 = 3, v29 = 0
    v30 = func_addr.i64 fn7
    v31 = iconst.i64 1
    v32 = iadd_imm v30, 3
    v33 = iconst.i64 3
    v34 = call fn8(v33)  ; v33 = 3
    store aligned v32, v34
    store aligned v31, v34+8  ; v31 = 1
    store aligned v2, v34+16
    v35 = iadd_imm v34, 1
    v36 = iconst.i64 3
    v37 = iconst.i64 1
    call fn9(v21, v35, v36, v37)  ; v36 = 3, v37 = 1
    v38 = load.i64 v21
    v39 = iadd_imm v19, 1
    v40 = iadd_imm v20, -8
    store aligned v39, v40
    v41 = func_addr.i64 fn11
    v42 = iconst.i64 1
    v43 = iadd_imm v41, 3
    v44 = iconst.i64 3
    v45 = call fn12(v44)  ; v44 = 3
    store aligned v43, v45
    store aligned v42, v45+8  ; v42 = 1
    store aligned v2, v45+16
    v46 = iadd_imm v45, 1
    stack_store v40, ss0
    v47 = stack_addr.i64 ss0
    v48 = call fn13(v46, v47)
    v49 = stack_load.i64 ss0
    v50 = isub v20, v49
    v51 = load.i64 v38+8
    v52 = ishl_imm v51, 3
    v53 = iadd v52, v50
    v54 = ushr_imm v53, 3
    store v54, v38+8
    return_call fn10(v49, v38, v48, v21)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:20 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
