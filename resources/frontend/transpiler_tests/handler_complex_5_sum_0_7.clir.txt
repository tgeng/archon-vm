CLIR
========
[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64) -> i64 tail
    sig3 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64, i64, i64) apple_aarch64
    sig6 = (i64, i64, i64, i64) -> i64 tail
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 apple_aarch64
    sig9 = (i64, i64, i64) -> i64 apple_aarch64
    sig10 = (i64, i64, i64) -> i64 apple_aarch64
    fn0 = u0:0 sig0
    fn1 = colocated u0:21 sig1
    fn2 = colocated u0:19 sig2
    fn3 = u0:6 sig3
    fn4 = colocated u0:23 sig4
    fn5 = u0:7 sig5
    fn6 = u0:12 sig6
    fn7 = colocated u0:25 sig7
    fn8 = u0:1 sig8
    fn9 = u0:3 sig9
    fn10 = u0:3 sig10

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 0
    v4 = call fn0(v3)  ; v3 = 0
    v5 = iadd_imm v4, 1
    v6 = iconst.i64 3
    v7 = iconst.i64 3
    v8 = func_addr.i64 fn1
    v9 = iadd_imm v8, 3
    v10 = func_addr.i64 fn2
    stack_store v0, ss0
    v11 = stack_addr.i64 ss0
    v12 = call fn3(v11, v2, v5, v6, v7, v9, v10)  ; v6 = 3, v7 = 3
    v13 = stack_load.i64 ss0
    v14 = load.i64 v12
    v15 = func_addr.i64 fn4
    v16 = iadd_imm v15, 3
    v17 = iconst.i64 3
    v18 = iconst.i64 0
    call fn5(v14, v16, v17, v18)  ; v17 = 3, v18 = 0
    v19 = load.i64 v14
    v20 = iadd_imm v12, 1
    v21 = iadd_imm v13, -8
    store aligned v20, v21
    v22 = func_addr.i64 fn7
    v23 = iadd_imm v22, 3
    stack_store v21, ss0
    v24 = stack_addr.i64 ss0
    v25 = call fn8(v23, v24)
    v26 = stack_load.i64 ss0
    v27 = isub v13, v26
    v28 = load.i64 v19+8
    v29 = ishl_imm v28, 3
    v30 = iadd v29, v27
    v31 = ushr_imm v30, 3
    store v31, v19+8
    v32 = call fn9(v0, v26, v25)
    v33 = call fn6(v26, v19, v25, v14)
    v34 = call fn10(v0, v26, v33)
    v35 = load.i64 v33
    v36 = iadd_imm v33, 8
    v37 = sshr_imm v35, 1
    return v37
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[main$__lambda_1__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 apple_aarch64
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64) -> i64 tail
    fn0 = u0:1 sig0
    fn1 = u0:1 sig2
    fn2 = u0:1 sig4

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block4

block4:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2, block3]

block1:
    v9 = iconst.i64 2
    v10 = ishl_imm v9, 1  ; v9 = 2
    v11 = load.i64 v0
    v12 = iadd_imm.i64 v5, -8
    store aligned v11, v12
    v13 = iadd_imm v12, -8
    store aligned v10, v13
    v14 = global_value.i64 gv0
    v15 = iadd_imm v14, 8
    v16 = load.i64 v0+8
    stack_store v13, ss0
    v17 = stack_addr.i64 ss0
    v18 = call fn0(v16, v17)
    v19 = stack_load.i64 ss0
    v20 = call_indirect sig1, v18(v19, v15)
    v21 = load.i64 v20
    v22 = iadd_imm v20, 8
    v23 = iconst.i64 0
    v24 = iadd_imm v21, -1
    v25 = ishl_imm v23, 3  ; v23 = 0
    v26 = iadd v24, v25
    v27 = load.i64 v26
    v28 = iconst.i64 1
    v29 = iadd_imm v21, -1
    v30 = ishl_imm v28, 3  ; v28 = 1
    v31 = iadd v29, v30
    v32 = load.i64 v31
    v33 = iconst.i64 0
    v34 = ishl_imm v33, 1  ; v33 = 0
    v35 = iconst.i64 0
    v36 = ishl_imm v35, 1  ; v35 = 0
    v37 = iadd_imm v22, -8
    store aligned v36, v37
    v38 = iadd_imm v37, -8
    store aligned v11, v38
    v39 = iadd_imm v38, -8
    store aligned v34, v39
    stack_store v39, ss0
    v40 = stack_addr.i64 ss0
    v41 = call fn1(v27, v40)
    v42 = stack_load.i64 ss0
    v43 = isub.i64 v0, v42
    v44 = ushr_imm v43, 3
    store v44, v1+8
    v45 = iconst.i64 1
    store v45, v1+24  ; v45 = 1
    store v21, v4
    store v27, v4+8
    store v32, v4+16
    return_call_indirect sig3, v41(v42, v1)

block2:
    v46 = iconst.i64 0
    v47 = ishl_imm v46, 1  ; v46 = 0
    v48 = load.i64 v0
    v49 = iconst.i64 1
    v50 = ishl_imm v49, 1  ; v49 = 1
    v51 = iadd_imm.i64 v5, -8
    store aligned v50, v51
    v52 = iadd_imm v51, -8
    store aligned v48, v52
    v53 = iadd_imm v52, -8
    store aligned v47, v53
    v54 = load.i64 v4+16
    stack_store v53, ss0
    v55 = stack_addr.i64 ss0
    v56 = call fn2(v54, v55)
    v57 = stack_load.i64 ss0
    v58 = isub.i64 v0, v57
    v59 = ushr_imm v58, 3
    store v59, v1+8
    v60 = iconst.i64 2
    store v60, v1+24  ; v60 = 2
    store.i64 v6, v4+24
    return_call_indirect sig5, v56(v57, v1)

block3:
    v61 = load.i64 v4+24
    v62 = sshr_imm v61, 1
    v63 = sshr_imm.i64 v6, 1
    v64 = iadd v62, v63
    v65 = ishl_imm v64, 1
    v66 = iadd_imm.i64 v0, 8
    store v65, v66
    v67 = load.i64 v1+16
    v68 = load.i64 v67+8
    v69 = ishl_imm v68, 3
    v70 = iadd.i64 v0, v69
    v71 = load.i64 v67
    return_call_indirect sig6, v71(v70, v67, v66)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:24 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 72
    v3 = call fn0(v2)  ; v2 = 72
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[main$__lambda_2__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:14 sig4
    fn4 = colocated u0:17 sig5
    fn5 = u0:4 sig6
    fn6 = colocated u0:14 sig8
    fn7 = colocated u0:17 sig9
    fn8 = u0:4 sig10

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block5

block5:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2, block3, block4]

block1:
    v9 = load.i64 v0
    v10 = isub.i64 v0, v5
    v11 = ushr_imm v10, 3
    store v11, v1+8
    v12 = iconst.i64 1
    store v12, v1+24  ; v12 = 1
    v13 = iconst.i64 0
    v14 = func_addr.i64 fn0
    v15 = func_addr.i64 fn1
    v16 = iconst.i64 0
    v17 = call fn2(v9, v16, v5, v1, v13, v14, v15)  ; v16 = 0, v13 = 0
    v18 = load.i64 v17
    v19 = load.i64 v17+8
    v20 = load.i64 v17+16
    return_call_indirect sig3, v18(v19, v20)

block2:
    v21 = load.i64 v0
    v22 = isub.i64 v0, v5
    v23 = ushr_imm v22, 3
    store v23, v1+8
    v24 = iconst.i64 2
    store v24, v1+24  ; v24 = 2
    store.i64 v6, v4
    v25 = iconst.i64 0
    v26 = func_addr.i64 fn3
    v27 = func_addr.i64 fn4
    v28 = iconst.i64 0
    v29 = call fn5(v21, v28, v5, v1, v25, v26, v27)  ; v28 = 0, v25 = 0
    v30 = load.i64 v29
    v31 = load.i64 v29+8
    v32 = load.i64 v29+16
    return_call_indirect sig7, v30(v31, v32)

block3:
    v33 = iconst.i64 2
    v34 = sshr_imm.i64 v6, 1
    v35 = imul v34, v33  ; v33 = 2
    v36 = load.i64 v4
    v37 = sshr_imm v36, 1
    v38 = iadd v37, v35
    v39 = load.i64 v0
    v40 = isub.i64 v0, v5
    v41 = ushr_imm v40, 3
    store v41, v1+8
    v42 = iconst.i64 3
    store v42, v1+24  ; v42 = 3
    store.i64 v6, v4+8
    v43 = ishl_imm v35, 1
    store v43, v4+16
    v44 = ishl_imm v38, 1
    store v44, v4+24
    v45 = iconst.i64 0
    v46 = func_addr.i64 fn6
    v47 = func_addr.i64 fn7
    v48 = iconst.i64 0
    v49 = call fn8(v39, v48, v5, v1, v45, v46, v47)  ; v48 = 0, v45 = 0
    v50 = load.i64 v49
    v51 = load.i64 v49+8
    v52 = load.i64 v49+16
    return_call_indirect sig11, v50(v51, v52)

block4:
    v53 = iconst.i64 4
    v54 = sshr_imm.i64 v6, 1
    v55 = imul v54, v53  ; v53 = 4
    v56 = load.i64 v4+24
    v57 = sshr_imm v56, 1
    v58 = iadd v57, v55
    v59 = ishl_imm v58, 1
    v60 = iadd_imm.i64 v0, 0
    store v59, v60
    v61 = load.i64 v1+16
    v62 = load.i64 v61+8
    v63 = ishl_imm v62, 3
    v64 = iadd.i64 v0, v63
    v65 = load.i64 v61
    return_call_indirect sig12, v65(v64, v61, v60)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:26 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 80
    v3 = call fn0(v2)  ; v2 = 80
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:20 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
