CLIR
========
[add__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = sshr_imm v2, 1
    v5 = sshr_imm v3, 1
    v6 = iadd v4, v5
    v7 = ishl_imm v6, 1
    v8 = iadd_imm v0, 8
    store v7, v8
    v9 = load.i64 v1+8
    v10 = ishl_imm v9, 3
    v11 = iadd v0, v10
    v12 = load.i64 v1
    return_call_indirect sig0, v12(v11, v1, v8)
}


[fold__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64) -> i64 tail
    sig3 = (i64, i64, i64) -> i64 tail
    fn0 = u0:1 sig0
    fn1 = colocated u0:24 sig2

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm eq v3, 2
    brif v7, block3, block7

block7:
    brif.i64 v3, block1, block1

block1:
    v8 = load.i64 v0+32
    v9 = load.i64 v0
    v10 = sshr_imm v8, 1
    v11 = sshr_imm v9, 1
    v12 = icmp slt v10, v11
    v13 = uextend.i32 v12
    br_table v13, block5, [block4, block2]

block2:
    v16 = load.i64 v0+8
    v17 = iadd_imm v16, -1
    v18 = sshr_imm.i64 v8, 1
    v19 = ishl_imm v18, 3
    v20 = iadd v17, v19
    v21 = load.i64 v20
    v22 = load.i64 v0+16
    v23 = iadd_imm.i64 v5, -8
    store aligned v21, v23
    v24 = iadd_imm v23, -8
    store aligned v22, v24
    v25 = load.i64 v0+24
    stack_store v24, ss0
    v26 = stack_addr.i64 ss0
    v27 = call fn0(v25, v26)
    v28 = stack_load.i64 ss0
    v29 = isub.i64 v0, v28
    v30 = ushr_imm v29, 3
    store v30, v1+8
    v31 = iconst.i64 2
    store v31, v1+24  ; v31 = 2
    v32 = ishl_imm.i8 v12, 1
    store v32, v4
    store v21, v4+8
    return_call_indirect sig1, v27(v28, v1)

block3:
    v33 = load.i64 v0+32
    v34 = iconst.i64 1
    v35 = sshr_imm v33, 1
    v36 = iadd v35, v34  ; v34 = 1
    v37 = load.i64 v0
    v38 = sshr_imm v37, 1
    v39 = load.i64 v0+8
    v40 = sshr_imm.i64 v6, 1
    v41 = load.i64 v0+24
    v42 = call fn1(v5, v38, v39, v40, v41, v36)
    jump block6(v42, v5)

block4:
    v43 = load.i64 v0+16
    v44 = sshr_imm v43, 1
    jump block6(v44, v5)

block5:
    trap unreachable

block6(v14: i64, v15: i64):
    v45 = ishl_imm v14, 1
    v46 = iadd_imm.i64 v0, 32
    store v45, v46
    v47 = load.i64 v1+16
    v48 = load.i64 v47+8
    v49 = ishl_imm v48, 3
    v50 = iadd.i64 v0, v49
    v51 = load.i64 v47
    return_call_indirect sig3, v51(v50, v47, v46)
}


[fold__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:23 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 64
    v3 = call fn0(v2)  ; v2 = 64
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[fold__specialized]
function u0:0(i64, i64, i64, i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64) -> i64 tail
    fn0 = u0:1 sig0
    fn1 = colocated u0:24 sig2

block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64):
    v6 = icmp slt v5, v1
    v9 = uextend.i32 v6
    br_table v9, block4, [block3, block2]

block3:
    jump block1(v3, v0)

block2:
    v10 = iadd_imm.i64 v2, -1
    v11 = ishl_imm.i64 v5, 3
    v12 = iadd v10, v11
    v13 = load.i64 v12
    v14 = ishl_imm.i64 v3, 1
    v15 = iadd_imm.i64 v0, -8
    store aligned v13, v15
    v16 = iadd_imm v15, -8
    store aligned v14, v16
    v17 = global_value.i64 gv0
    v18 = iadd_imm v17, 8
    stack_store v16, ss0
    v19 = stack_addr.i64 ss0
    v20 = call fn0(v4, v19)
    v21 = stack_load.i64 ss0
    v22 = call_indirect sig1, v20(v21, v18)
    v23 = load.i64 v22
    v24 = iadd_imm v22, 8
    v25 = iconst.i64 1
    v26 = iadd.i64 v5, v25  ; v25 = 1
    v27 = sshr_imm v23, 1
    return_call fn1(v24, v1, v2, v27, v4, v26)

block4:
    trap unreachable

block1(v7: i64, v8: i64):
    return v7
}


[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64) -> i64 tail
    sig3 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig4 = (i64, i64, i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64) -> i64 apple_aarch64
    sig9 = (i64, i64) -> i64 apple_aarch64
    fn0 = u0:0 sig0
    fn1 = colocated u0:26 sig1
    fn2 = colocated u0:19 sig2
    fn3 = u0:6 sig3
    fn4 = u0:12 sig4
    fn5 = colocated u0:22 sig5
    fn6 = u0:0 sig6
    fn7 = colocated u0:20 sig7
    fn8 = u0:0 sig8
    fn9 = u0:1 sig9

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 0
    v4 = call fn0(v3)  ; v3 = 0
    v5 = iadd_imm v4, 1
    v6 = iconst.i64 3
    v7 = iconst.i64 3
    v8 = func_addr.i64 fn1
    v9 = iadd_imm v8, 3
    v10 = func_addr.i64 fn2
    stack_store v0, ss0
    v11 = stack_addr.i64 ss0
    v12 = call fn3(v11, v2, v5, v6, v7, v9, v10)  ; v6 = 3, v7 = 3
    v13 = stack_load.i64 ss0
    v14 = load.i64 v12
    v15 = load.i64 v14
    v16 = func_addr.i64 fn5
    v17 = iconst.i64 5
    v18 = iadd_imm v16, 3
    v19 = iconst.i64 3
    v20 = ishl_imm v19, 1  ; v19 = 3
    v21 = iconst.i64 1
    v22 = ishl_imm v21, 1  ; v21 = 1
    v23 = iconst.i64 2
    v24 = ishl_imm v23, 1  ; v23 = 2
    v25 = iconst.i64 3
    v26 = ishl_imm v25, 1  ; v25 = 3
    v27 = iconst.i64 3
    v28 = call fn6(v27)  ; v27 = 3
    store aligned v22, v28
    store aligned v24, v28+8
    store aligned v26, v28+16
    v29 = iadd_imm v28, 1
    v30 = iconst.i64 10
    v31 = ishl_imm v30, 1  ; v30 = 10
    v32 = func_addr.i64 fn7
    v33 = iadd_imm v32, 3
    v34 = iconst.i64 0
    v35 = ishl_imm v34, 1  ; v34 = 0
    v36 = iconst.i64 7
    v37 = call fn8(v36)  ; v36 = 7
    store aligned v18, v37
    store aligned v17, v37+8  ; v17 = 5
    store aligned v20, v37+16
    store aligned v29, v37+24
    store aligned v31, v37+32
    store aligned v33, v37+40
    store aligned v35, v37+48
    v38 = iadd_imm v37, 1
    stack_store v13, ss0
    v39 = stack_addr.i64 ss0
    v40 = call fn9(v38, v39)
    v41 = stack_load.i64 ss0
    v42 = iadd_imm v12, 1
    v43 = iadd_imm v41, -8
    store aligned v42, v43
    v44 = isub v13, v43
    v45 = load.i64 v15+8
    v46 = ishl_imm v45, 3
    v47 = iadd v46, v44
    v48 = ushr_imm v47, 3
    store v48, v15+8
    v49 = call fn4(v43, v15, v40, v14)
    v50 = load.i64 v49
    v51 = iadd_imm v49, 8
    v52 = sshr_imm v50, 1
    return v52
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:25 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
