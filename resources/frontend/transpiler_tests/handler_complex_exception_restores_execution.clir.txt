CLIR
========
[interruptAndRecover__specialized]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64) -> i64 tail
    sig3 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64, i64, i64) apple_aarch64
    sig6 = (i64, i64, i64, i64) -> i64 tail
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64) -> i64 apple_aarch64
    sig9 = (i64, i64) -> i64 apple_aarch64
    fn0 = u0:0 sig0
    fn1 = colocated u0:21 sig1
    fn2 = colocated u0:19 sig2
    fn3 = u0:6 sig3
    fn4 = colocated u0:23 sig4
    fn5 = u0:7 sig5
    fn6 = u0:12 sig6
    fn7 = colocated u0:25 sig7
    fn8 = u0:0 sig8
    fn9 = u0:1 sig9

block0(v0: i64, v1: i64, v2: i64):
    v3 = global_value.i64 gv0
    v4 = iadd_imm v3, 8
    v5 = iconst.i64 0
    v6 = call fn0(v5)  ; v5 = 0
    v7 = iadd_imm v6, 1
    v8 = iconst.i64 3
    v9 = iconst.i64 3
    v10 = func_addr.i64 fn1
    v11 = iadd_imm v10, 3
    v12 = func_addr.i64 fn2
    stack_store v0, ss0
    v13 = stack_addr.i64 ss0
    v14 = call fn3(v13, v4, v7, v8, v9, v11, v12)  ; v8 = 3, v9 = 3
    v15 = stack_load.i64 ss0
    v16 = load.i64 v14
    v17 = func_addr.i64 fn4
    v18 = iadd_imm v17, 3
    v19 = iconst.i64 3
    v20 = iconst.i64 0
    call fn5(v16, v18, v19, v20)  ; v19 = 3, v20 = 0
    v21 = load.i64 v16
    v22 = func_addr.i64 fn7
    v23 = iconst.i64 2
    v24 = iadd_imm v22, 3
    v25 = ishl_imm v2, 1
    v26 = iconst.i64 4
    v27 = call fn8(v26)  ; v26 = 4
    store aligned v24, v27
    store aligned v23, v27+8  ; v23 = 2
    store aligned v1, v27+16
    store aligned v25, v27+24
    v28 = iadd_imm v27, 1
    stack_store v15, ss0
    v29 = stack_addr.i64 ss0
    v30 = call fn9(v28, v29)
    v31 = stack_load.i64 ss0
    v32 = iadd_imm v14, 1
    v33 = iadd_imm v31, -8
    store aligned v32, v33
    v34 = isub v15, v33
    v35 = load.i64 v21+8
    v36 = ishl_imm v35, 3
    v37 = iadd v36, v34
    v38 = ushr_imm v37, 3
    store v38, v21+8
    v39 = call fn6(v33, v21, v30, v16)
    v40 = load.i64 v39
    v41 = iadd_imm v39, 8
    v42 = sshr_imm v40, 1
    return v42
}


[interruptAndRecover$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[interruptAndRecover$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:1 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = load.i64 v0+16
    v5 = iconst.i64 1
    v6 = ishl_imm v5, 1  ; v5 = 1
    v7 = iadd_imm v0, -8
    store aligned v2, v7
    v8 = iadd_imm v7, -8
    store aligned v6, v8
    v9 = global_value.i64 gv0
    v10 = iadd_imm v9, 8
    stack_store v8, ss0
    v11 = stack_addr.i64 ss0
    v12 = call fn0(v4, v11)
    v13 = stack_load.i64 ss0
    v14 = call_indirect sig1, v12(v13, v10)
    v15 = load.i64 v14
    v16 = iadd_imm v14, 8
    v17 = iadd_imm v0, 16
    store v3, v17
    v18 = load.i64 v1+8
    v19 = ishl_imm v18, 3
    v20 = iadd v0, v19
    v21 = load.i64 v1
    return_call_indirect sig2, v21(v20, v1, v17)
}


[interruptAndRecover$__lambda_2__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname3
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:14 sig4
    fn4 = colocated u0:17 sig5
    fn5 = u0:4 sig6

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block3

block3:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2]

block1:
    v9 = load.i64 v0+16
    v10 = load.i64 v0+8
    v11 = iadd_imm.i64 v5, -8
    store aligned v10, v11
    v12 = isub.i64 v0, v11
    v13 = ushr_imm v12, 3
    store v13, v1+8
    v14 = iconst.i64 1
    store v14, v1+24  ; v14 = 1
    v15 = iconst.i64 1
    v16 = func_addr.i64 fn0
    v17 = func_addr.i64 fn1
    v18 = iconst.i64 0
    v19 = call fn2(v9, v18, v11, v1, v15, v16, v17)  ; v18 = 0, v15 = 1
    v20 = load.i64 v19
    v21 = load.i64 v19+8
    v22 = load.i64 v19+16
    return_call_indirect sig3, v20(v21, v22)

block2:
    v23 = load.i64 v0
    v24 = global_value.i64 gv0
    v25 = iadd_imm v24, 8
    v26 = iconst.i64 31
    v27 = ishl_imm v26, 1  ; v26 = 31
    v28 = iadd_imm.i64 v5, -8
    store aligned v27, v28
    v29 = iconst.i64 1
    v30 = func_addr.i64 fn3
    v31 = func_addr.i64 fn4
    v32 = iconst.i64 1
    v33 = call fn5(v23, v32, v28, v25, v29, v30, v31)  ; v32 = 1, v29 = 1
    v34 = load.i64 v33
    v35 = load.i64 v33+8
    v36 = load.i64 v33+16
    v37 = call_indirect sig7, v34(v35, v36)
    v38 = load.i64 v37
    v39 = iadd_imm v37, 8
    v40 = iconst.i64 31
    v41 = ishl_imm v40, 1  ; v40 = 31
    v42 = iadd_imm.i64 v0, 16
    store v41, v42
    v43 = load.i64 v1+16
    v44 = load.i64 v43+8
    v45 = ishl_imm v44, 3
    v46 = iadd.i64 v0, v45
    v47 = load.i64 v43
    return_call_indirect sig8, v47(v46, v43, v42)
}


[interruptAndRecover$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:26 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 48
    v3 = call fn0(v2)  ; v2 = 48
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64, i64) apple_aarch64
    sig5 = (i64, i64, i64, i64) -> i64 tail
    sig6 = (i64, i64) -> i64 tail
    sig7 = (i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:28 sig0
    fn1 = colocated u0:19 sig1
    fn2 = u0:6 sig2
    fn3 = colocated u0:30 sig3
    fn4 = u0:7 sig4
    fn5 = u0:12 sig5
    fn6 = colocated u0:32 sig6
    fn7 = u0:1 sig7

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 1
    v4 = ishl_imm v3, 1  ; v3 = 1
    v5 = iconst.i64 3
    v6 = iconst.i64 3
    v7 = func_addr.i64 fn0
    v8 = iadd_imm v7, 3
    v9 = func_addr.i64 fn1
    stack_store v0, ss0
    v10 = stack_addr.i64 ss0
    v11 = call fn2(v10, v2, v4, v5, v6, v8, v9)  ; v5 = 3, v6 = 3
    v12 = stack_load.i64 ss0
    v13 = load.i64 v11
    v14 = func_addr.i64 fn3
    v15 = iadd_imm v14, 3
    v16 = iconst.i64 2
    v17 = iconst.i64 1
    call fn4(v13, v15, v16, v17)  ; v16 = 2, v17 = 1
    v18 = load.i64 v13
    v19 = func_addr.i64 fn6
    v20 = iadd_imm v19, 3
    stack_store v12, ss0
    v21 = stack_addr.i64 ss0
    v22 = call fn7(v20, v21)
    v23 = stack_load.i64 ss0
    v24 = iadd_imm v11, 1
    v25 = iadd_imm v23, -8
    store aligned v24, v25
    v26 = isub v12, v25
    v27 = load.i64 v18+8
    v28 = ishl_imm v27, 3
    v29 = iadd v28, v26
    v30 = ushr_imm v29, 3
    store v30, v18+8
    v31 = call fn5(v25, v18, v22, v13)
    v32 = load.i64 v31
    v33 = iadd_imm v31, 8
    v34 = sshr_imm v32, 1
    return v34
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 100
    v5 = sshr_imm v2, 1
    v6 = imul v5, v4  ; v4 = 100
    v7 = sshr_imm v3, 1
    v8 = iadd v6, v7
    v9 = ishl_imm v8, 1
    v10 = iadd_imm v0, 8
    store v9, v10
    v11 = load.i64 v1+8
    v12 = ishl_imm v11, 3
    v13 = iadd v0, v12
    v14 = load.i64 v1
    return_call_indirect sig0, v14(v13, v1, v10)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64) -> i64 apple_aarch64
    sig3 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1
    fn2 = u0:0 sig2

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = sshr_imm v2, 1
    v5 = sshr_imm v3, 1
    v6 = imul v4, v5
    v7 = ishl_imm v6, 1
    v8 = iconst.i64 1
    v9 = ishl_imm v8, 1  ; v8 = 1
    v10 = iconst.i64 0
    v11 = call fn0(v10)  ; v10 = 0
    v12 = iadd_imm v11, 1
    v13 = iconst.i64 2
    v14 = call fn1(v13)  ; v13 = 2
    store aligned v9, v14
    store aligned v12, v14+8
    v15 = iadd_imm v14, 1
    v16 = iconst.i64 2
    v17 = call fn2(v16)  ; v16 = 2
    store aligned v7, v17
    store aligned v15, v17+8
    v18 = iadd_imm v17, 1
    v19 = iadd_imm v0, 8
    store v18, v19
    v20 = load.i64 v1+8
    v21 = ishl_imm v20, 3
    v22 = iadd v0, v21
    v23 = load.i64 v1
    return_call_indirect sig3, v23(v22, v1, v19)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname0
    gv2 = symbol colocated userextname0
    gv3 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64) -> i64 tail
    sig7 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64, i64) -> i64 tail
    sig10 = (i64, i64) -> i64 tail
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig13 = (i64, i64) -> i64 tail
    sig14 = (i64, i64, i64) -> i64 tail
    sig15 = (i64, i64) -> i64 tail
    sig16 = (i64, i64) -> i64 tail
    sig17 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig18 = (i64, i64) -> i64 tail
    sig19 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:20 sig4
    fn4 = colocated u0:14 sig5
    fn5 = colocated u0:17 sig6
    fn6 = u0:4 sig7
    fn7 = colocated u0:20 sig9
    fn8 = colocated u0:14 sig10
    fn9 = colocated u0:17 sig11
    fn10 = u0:4 sig12
    fn11 = colocated u0:20 sig14
    fn12 = colocated u0:14 sig15
    fn13 = colocated u0:17 sig16
    fn14 = u0:4 sig17

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = global_value.i64 gv0
    v4 = iadd_imm v3, 8
    v5 = iconst.i64 2
    v6 = ishl_imm v5, 1  ; v5 = 2
    v7 = iadd_imm v0, -8
    store aligned v6, v7
    v8 = iconst.i64 1
    v9 = func_addr.i64 fn0
    v10 = func_addr.i64 fn1
    v11 = iconst.i64 1
    v12 = call fn2(v2, v11, v7, v4, v8, v9, v10)  ; v11 = 1, v8 = 1
    v13 = load.i64 v12
    v14 = load.i64 v12+8
    v15 = load.i64 v12+16
    v16 = call_indirect sig3, v13(v14, v15)
    v17 = load.i64 v16
    v18 = iadd_imm v16, 8
    v19 = iconst.i64 3
    v20 = call fn3(v18, v2, v19)  ; v19 = 3
    v21 = global_value.i64 gv1
    v22 = iadd_imm v21, 8
    v23 = ishl_imm v20, 1
    v24 = iadd_imm v18, -8
    store aligned v23, v24
    v25 = iconst.i64 1
    v26 = func_addr.i64 fn4
    v27 = func_addr.i64 fn5
    v28 = iconst.i64 1
    v29 = call fn6(v2, v28, v24, v22, v25, v26, v27)  ; v28 = 1, v25 = 1
    v30 = load.i64 v29
    v31 = load.i64 v29+8
    v32 = load.i64 v29+16
    v33 = call_indirect sig8, v30(v31, v32)
    v34 = load.i64 v33
    v35 = iadd_imm v33, 8
    v36 = iconst.i64 5
    v37 = call fn7(v35, v2, v36)  ; v36 = 5
    v38 = global_value.i64 gv2
    v39 = iadd_imm v38, 8
    v40 = ishl_imm v37, 1
    v41 = iadd_imm v35, -8
    store aligned v40, v41
    v42 = iconst.i64 1
    v43 = func_addr.i64 fn8
    v44 = func_addr.i64 fn9
    v45 = iconst.i64 1
    v46 = call fn10(v2, v45, v41, v39, v42, v43, v44)  ; v45 = 1, v42 = 1
    v47 = load.i64 v46
    v48 = load.i64 v46+8
    v49 = load.i64 v46+16
    v50 = call_indirect sig13, v47(v48, v49)
    v51 = load.i64 v50
    v52 = iadd_imm v50, 8
    v53 = iconst.i64 7
    v54 = call fn11(v52, v2, v53)  ; v53 = 7
    v55 = global_value.i64 gv3
    v56 = iadd_imm v55, 8
    v57 = ishl_imm v54, 1
    v58 = iadd_imm v52, -8
    store aligned v57, v58
    v59 = iconst.i64 1
    v60 = func_addr.i64 fn12
    v61 = func_addr.i64 fn13
    v62 = iconst.i64 1
    v63 = call fn14(v2, v62, v58, v56, v59, v60, v61)  ; v62 = 1, v59 = 1
    v64 = load.i64 v63
    v65 = load.i64 v63+8
    v66 = load.i64 v63+16
    v67 = call_indirect sig18, v64(v65, v66)
    v68 = load.i64 v67
    v69 = iadd_imm v67, 8
    v70 = iconst.i64 1
    v71 = ishl_imm v70, 1  ; v70 = 1
    v72 = iadd_imm v0, 0
    store v71, v72
    v73 = load.i64 v1+8
    v74 = ishl_imm v73, 3
    v75 = iadd v0, v74
    v76 = load.i64 v1
    return_call_indirect sig19, v76(v75, v1, v72)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:27 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
