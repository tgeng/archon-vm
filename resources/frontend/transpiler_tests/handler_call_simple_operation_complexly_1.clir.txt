CLIR
========
[call__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname0
    gv2 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:15 sig0
    fn1 = colocated u0:18 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:15 sig4
    fn4 = colocated u0:18 sig5
    fn5 = u0:4 sig6
    fn6 = colocated u0:15 sig8
    fn7 = colocated u0:18 sig9
    fn8 = u0:4 sig10

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block5

block5:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2, block3, block4]

block1:
    v9 = symbol_value.i64 gv0
    v10 = iadd_imm v9, 8
    v11 = iadd_imm v10, 3
    v12 = load.i64 v0
    v13 = iadd_imm.i64 v5, -8
    store aligned v12, v13
    v14 = isub.i64 v0, v13
    v15 = ushr_imm v14, 3
    store v15, v1+8
    v16 = iconst.i64 1
    store v16, v1+24  ; v16 = 1
    v17 = iconst.i64 1
    v18 = func_addr.i64 fn0
    v19 = func_addr.i64 fn1
    v20 = iconst.i64 1
    v21 = call fn2(v11, v13, v1, v17, v18, v19, v20)  ; v17 = 1, v20 = 1
    v22 = load.i64 v21
    v23 = load.i64 v21+8
    v24 = load.i64 v21+16
    return_call_indirect sig3, v22(v23, v24)

block2:
    v25 = symbol_value.i64 gv1
    v26 = iadd_imm v25, 8
    v27 = iadd_imm v26, 3
    v28 = load.i64 v0+8
    v29 = iadd_imm.i64 v5, -8
    store aligned v28, v29
    v30 = isub.i64 v0, v29
    v31 = ushr_imm v30, 3
    store v31, v1+8
    v32 = iconst.i64 2
    store v32, v1+24  ; v32 = 2
    store.i64 v6, v4
    v33 = iconst.i64 1
    v34 = func_addr.i64 fn3
    v35 = func_addr.i64 fn4
    v36 = iconst.i64 1
    v37 = call fn5(v27, v29, v1, v33, v34, v35, v36)  ; v33 = 1, v36 = 1
    v38 = load.i64 v37
    v39 = load.i64 v37+8
    v40 = load.i64 v37+16
    return_call_indirect sig7, v38(v39, v40)

block3:
    v41 = load.i64 v4
    v42 = sshr_imm v41, 1
    v43 = sshr_imm.i64 v6, 1
    v44 = iadd v42, v43
    v45 = symbol_value.i64 gv2
    v46 = iadd_imm v45, 8
    v47 = iadd_imm v46, 3
    v48 = load.i64 v0+16
    v49 = iadd_imm.i64 v5, -8
    store aligned v48, v49
    v50 = isub.i64 v0, v49
    v51 = ushr_imm v50, 3
    store v51, v1+8
    v52 = iconst.i64 3
    store v52, v1+24  ; v52 = 3
    store.i64 v6, v4+8
    v53 = ishl_imm v44, 1
    store v53, v4+16
    v54 = iconst.i64 1
    v55 = func_addr.i64 fn6
    v56 = func_addr.i64 fn7
    v57 = iconst.i64 1
    v58 = call fn8(v47, v49, v1, v54, v55, v56, v57)  ; v54 = 1, v57 = 1
    v59 = load.i64 v58
    v60 = load.i64 v58+8
    v61 = load.i64 v58+16
    return_call_indirect sig11, v59(v60, v61)

block4:
    v62 = load.i64 v4+16
    v63 = sshr_imm v62, 1
    v64 = sshr_imm.i64 v6, 1
    v65 = iadd v63, v64
    v66 = ishl_imm v65, 1
    v67 = iadd_imm.i64 v0, 16
    store v66, v67
    v68 = load.i64 v1+16
    v69 = load.i64 v68+8
    v70 = ishl_imm v69, 3
    v71 = iadd.i64 v0, v70
    v72 = load.i64 v68
    return_call_indirect sig12, v72(v71, v68, v67)
}


[call__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:22 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 64
    v3 = call fn0(v2)  ; v2 = 64
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    sig5 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig6 = (i64, i64, i64) apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64, i64) apple_aarch64
    sig9 = (i64, i64, i64, i64) -> i64 tail
    sig10 = (i64, i64) -> i64 tail
    sig11 = (i64) -> i64 apple_aarch64
    sig12 = (i64, i64) -> i64 apple_aarch64
    fn0 = u0:0 sig0
    fn1 = colocated u0:24 sig1
    fn2 = colocated u0:26 sig2
    fn3 = colocated u0:28 sig3
    fn4 = colocated u0:20 sig4
    fn5 = u0:6 sig5
    fn6 = u0:7 sig6
    fn7 = colocated u0:30 sig7
    fn8 = u0:8 sig8
    fn9 = u0:13 sig9
    fn10 = colocated u0:21 sig10
    fn11 = u0:0 sig11
    fn12 = u0:1 sig12

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 0
    v4 = call fn0(v3)  ; v3 = 0
    v5 = iadd_imm v4, 1
    v6 = func_addr.i64 fn1
    v7 = iadd_imm v6, 3
    v8 = func_addr.i64 fn2
    v9 = iadd_imm v8, 3
    v10 = func_addr.i64 fn3
    v11 = iadd_imm v10, 3
    v12 = func_addr.i64 fn4
    stack_store v0, ss0
    v13 = stack_addr.i64 ss0
    v14 = call fn5(v13, v2, v5, v7, v9, v11, v12)
    v15 = stack_load.i64 ss0
    v16 = symbol_value.i64 gv1
    v17 = iadd_imm v16, 8
    v18 = iadd_imm v17, 3
    v19 = func_addr.i64 fn7
    v20 = iadd_imm v19, 3
    call fn6(v14, v18, v20)
    v21 = load.i64 v14
    v22 = func_addr.i64 fn10
    v23 = iconst.i64 3
    v24 = iadd_imm v22, 3
    v25 = iconst.i64 1
    v26 = ishl_imm v25, 1  ; v25 = 1
    v27 = iconst.i64 2
    v28 = ishl_imm v27, 1  ; v27 = 2
    v29 = iconst.i64 3
    v30 = ishl_imm v29, 1  ; v29 = 3
    v31 = iconst.i64 5
    v32 = call fn11(v31)  ; v31 = 5
    store aligned v24, v32
    store aligned v23, v32+8  ; v23 = 3
    store aligned v26, v32+16
    store aligned v28, v32+24
    store aligned v30, v32+32
    v33 = iadd_imm v32, 1
    stack_store v15, ss0
    v34 = stack_addr.i64 ss0
    v35 = call fn12(v33, v34)
    v36 = stack_load.i64 ss0
    v37 = isub v15, v36
    v38 = load.i64 v21+8
    v39 = ishl_imm v38, 3
    v40 = iadd v39, v37
    v41 = ushr_imm v40, 3
    store v41, v21+8
    v42 = call fn9(v36, v21, v35, v14)
    v43 = load.i64 v42
    v44 = iadd_imm v42, 8
    v45 = sshr_imm v43, 1
    return v45
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 0
    v4 = call fn0(v3)  ; v3 = 0
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 2
    v4 = call fn0(v3)  ; v3 = 2
    store aligned v2, v4
    store aligned v2, v4+8
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[main$__lambda_3__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 2
    v5 = sshr_imm v3, 1
    v6 = imul v5, v4  ; v4 = 2
    v7 = iconst.i64 1
    v8 = ishl_imm v7, 1  ; v7 = 1
    v9 = ishl_imm v6, 1
    v10 = iconst.i64 2
    v11 = call fn0(v10)  ; v10 = 2
    store aligned v8, v11
    store aligned v9, v11+8
    v12 = iadd_imm v11, 1
    v13 = iconst.i64 2
    v14 = call fn1(v13)  ; v13 = 2
    store aligned v2, v14
    store aligned v12, v14+8
    v15 = iadd_imm v14, 1
    v16 = iadd_imm v0, 8
    store v15, v16
    v17 = load.i64 v1+8
    v18 = ishl_imm v17, 3
    v19 = iadd v0, v18
    v20 = load.i64 v1
    return_call_indirect sig2, v20(v19, v1, v16)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:23 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
