CLIR
========
[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname8
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    sig5 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig6 = (i64, i64, i64) apple_aarch64
    sig7 = (i64, i64, i64) apple_aarch64
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64, i64, i64) -> i64 tail
    sig10 = (i64, i64) -> i64 tail
    sig11 = (i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:23 sig1
    fn2 = colocated u0:25 sig2
    fn3 = colocated u0:27 sig3
    fn4 = colocated u0:21 sig4
    fn5 = u0:6 sig5
    fn6 = u0:7 sig6
    fn7 = u0:8 sig7
    fn8 = colocated u0:29 sig8
    fn9 = u0:13 sig9
    fn10 = colocated u0:31 sig10
    fn11 = u0:1 sig11

block0(v0: i64):
    v1 = call fn0()
    v2 = iconst.i64 1
    v3 = ishl_imm v2, 1  ; v2 = 1
    v4 = func_addr.i64 fn1
    v5 = iadd_imm v4, 3
    v6 = func_addr.i64 fn2
    v7 = iadd_imm v6, 3
    v8 = func_addr.i64 fn3
    v9 = iadd_imm v8, 3
    v10 = func_addr.i64 fn4
    stack_store v0, ss0
    v11 = stack_addr.i64 ss0
    v12 = call fn5(v11, v1, v3, v5, v7, v9, v10)
    v13 = stack_load.i64 ss0
    v14 = symbol_value.i64 gv0
    v15 = iadd_imm v14, 8
    v16 = iadd_imm v15, 3
    v17 = func_addr.i64 fn8
    v18 = iadd_imm v17, 3
    call fn7(v12, v16, v18)
    v19 = load.i64 v12
    v20 = func_addr.i64 fn10
    v21 = iadd_imm v20, 3
    stack_store v13, ss0
    v22 = stack_addr.i64 ss0
    v23 = call fn11(v21, v22)
    v24 = stack_load.i64 ss0
    v25 = isub v13, v24
    v26 = load.i64 v19+8
    v27 = ishl_imm v26, 3
    v28 = iadd v27, v25
    v29 = ushr_imm v28, 3
    store v29, v19+8
    v30 = call fn9(v24, v19, v23, v12)
    v31 = load.i64 v30
    v32 = iadd_imm v30, 8
    v33 = sshr_imm v31, 1
    return v33
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 0
    v4 = call fn0(v3)  ; v3 = 0
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 2
    v4 = call fn0(v3)  ; v3 = 2
    store aligned v2, v4
    store aligned v2, v4+8
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[main$__lambda_3__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 apple_aarch64
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64) -> i64 apple_aarch64
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 apple_aarch64
    sig6 = (i64, i64) -> i64 tail
    sig7 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = u0:1 sig1
    fn2 = u0:1 sig3
    fn3 = u0:1 sig5

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block4

block4:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2, block3]

block1:
    v9 = iconst.i64 2
    v10 = ishl_imm v9, 1  ; v9 = 2
    v11 = load.i64 v0
    v12 = iadd_imm.i64 v5, -8
    store aligned v11, v12
    v13 = iadd_imm v12, -8
    store aligned v10, v13
    v14 = call fn0()
    v15 = load.i64 v0+8
    stack_store v13, ss0
    v16 = stack_addr.i64 ss0
    v17 = call fn1(v15, v16)
    v18 = stack_load.i64 ss0
    v19 = call_indirect sig2, v17(v18, v14)
    v20 = load.i64 v19
    v21 = iadd_imm v19, 8
    v22 = iconst.i64 0
    v23 = iadd_imm v20, -1
    v24 = ishl_imm v22, 3  ; v22 = 0
    v25 = iadd v23, v24
    v26 = load.i64 v25
    v27 = iconst.i64 1
    v28 = iadd_imm v20, -1
    v29 = ishl_imm v27, 3  ; v27 = 1
    v30 = iadd v28, v29
    v31 = load.i64 v30
    v32 = iconst.i64 0
    v33 = ishl_imm v32, 1  ; v32 = 0
    v34 = iconst.i64 3
    v35 = ishl_imm v34, 1  ; v34 = 3
    v36 = iadd_imm v21, -8
    store aligned v35, v36
    v37 = iadd_imm v36, -8
    store aligned v11, v37
    v38 = iadd_imm v37, -8
    store aligned v33, v38
    stack_store v38, ss0
    v39 = stack_addr.i64 ss0
    v40 = call fn2(v26, v39)
    v41 = stack_load.i64 ss0
    v42 = isub.i64 v0, v41
    v43 = ushr_imm v42, 3
    store v43, v1+8
    v44 = iconst.i64 1
    store v44, v1+24  ; v44 = 1
    store v20, v4
    store v26, v4+8
    store v31, v4+16
    return_call_indirect sig4, v40(v41, v1)

block2:
    v45 = iconst.i64 0
    v46 = ishl_imm v45, 1  ; v45 = 0
    v47 = load.i64 v0
    v48 = iconst.i64 5
    v49 = ishl_imm v48, 1  ; v48 = 5
    v50 = iadd_imm.i64 v5, -8
    store aligned v49, v50
    v51 = iadd_imm v50, -8
    store aligned v47, v51
    v52 = iadd_imm v51, -8
    store aligned v46, v52
    v53 = load.i64 v4+16
    stack_store v52, ss0
    v54 = stack_addr.i64 ss0
    v55 = call fn3(v53, v54)
    v56 = stack_load.i64 ss0
    v57 = isub.i64 v0, v56
    v58 = ushr_imm v57, 3
    store v58, v1+8
    v59 = iconst.i64 2
    store v59, v1+24  ; v59 = 2
    store.i64 v6, v4+24
    return_call_indirect sig6, v55(v56, v1)

block3:
    v60 = load.i64 v4+24
    v61 = sshr_imm v60, 1
    v62 = sshr_imm.i64 v6, 1
    v63 = iadd v61, v62
    v64 = ishl_imm v63, 1
    v65 = iadd_imm.i64 v0, 8
    store v64, v65
    v66 = load.i64 v1+16
    v67 = load.i64 v66+8
    v68 = ishl_imm v67, 3
    v69 = iadd.i64 v0, v68
    v70 = load.i64 v66
    return_call_indirect sig7, v70(v69, v66, v65)
}


[main$__lambda_3__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:30 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 72
    v3 = call fn0(v2)  ; v2 = 72
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[main$__lambda_4__cps_impl]
function u0:0(i64, i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:16 sig0
    fn1 = colocated u0:19 sig1
    fn2 = u0:4 sig2

block0(v0: i64, v1: i64, v2: i64):
    v3 = load.i64 v1+24
    v4 = iadd_imm v1, 32
    v5 = iadd_imm v2, 8
    v6 = load.i64 v2
    v7 = icmp_imm ugt v3, 0xffff_ffff
    brif v7, block1, block3

block3:
    v8 = ireduce.i32 v3
    br_table v8, block1, [block1, block2]

block1:
    v9 = symbol_value.i64 gv0
    v10 = iadd_imm v9, 8
    v11 = iadd_imm v10, 3
    v12 = isub.i64 v0, v5
    v13 = ushr_imm v12, 3
    store v13, v1+8
    v14 = iconst.i64 1
    store v14, v1+24  ; v14 = 1
    v15 = iconst.i64 0
    v16 = func_addr.i64 fn0
    v17 = func_addr.i64 fn1
    v18 = iconst.i64 1
    v19 = call fn2(v11, v5, v1, v15, v16, v17, v18)  ; v15 = 0, v18 = 1
    v20 = load.i64 v19
    v21 = load.i64 v19+8
    v22 = load.i64 v19+16
    return_call_indirect sig3, v20(v21, v22)

block2:
    v23 = iconst.i64 2
    v24 = sshr_imm.i64 v6, 1
    v25 = imul v24, v23  ; v23 = 2
    v26 = ishl_imm v25, 1
    v27 = iadd_imm.i64 v0, -8
    store v26, v27
    v28 = load.i64 v1+16
    v29 = load.i64 v28+8
    v30 = ishl_imm v29, 3
    v31 = iadd.i64 v0, v30
    v32 = load.i64 v28
    return_call_indirect sig4, v32(v31, v28, v27)
}


[main$__lambda_4__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = colocated u0:32 sig1

block0(v0: i64, v1: i64):
    v2 = iconst.i64 40
    v3 = call fn0(v2)  ; v2 = 40
    v4 = func_addr.i64 fn1
    store v4, v3
    store v1, v3+16
    v5 = iadd_imm v0, -8
    return_call fn1(v0, v3, v5)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:22 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
