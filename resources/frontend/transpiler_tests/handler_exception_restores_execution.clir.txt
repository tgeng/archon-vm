CLIR
========
[interruptAndRecover__specialized]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    sig5 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig6 = (i64, i64, i64) apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64, i64) apple_aarch64
    sig9 = (i64, i64, i64, i64) -> i64 tail
    sig10 = (i64, i64) -> i64 tail
    sig11 = (i64) -> i64 apple_aarch64
    sig12 = (i64, i64) -> i64 apple_aarch64
    fn0 = u0:0 sig0
    fn1 = colocated u0:22 sig1
    fn2 = colocated u0:24 sig2
    fn3 = colocated u0:26 sig3
    fn4 = colocated u0:20 sig4
    fn5 = u0:6 sig5
    fn6 = u0:7 sig6
    fn7 = colocated u0:28 sig7
    fn8 = u0:8 sig8
    fn9 = u0:13 sig9
    fn10 = colocated u0:30 sig10
    fn11 = u0:0 sig11
    fn12 = u0:1 sig12

block0(v0: i64, v1: i64):
    v2 = global_value.i64 gv0
    v3 = iadd_imm v2, 8
    v4 = iconst.i64 0
    v5 = call fn0(v4)  ; v4 = 0
    v6 = iadd_imm v5, 1
    v7 = func_addr.i64 fn1
    v8 = iadd_imm v7, 3
    v9 = func_addr.i64 fn2
    v10 = iadd_imm v9, 3
    v11 = func_addr.i64 fn3
    v12 = iadd_imm v11, 3
    v13 = func_addr.i64 fn4
    stack_store v0, ss0
    v14 = stack_addr.i64 ss0
    v15 = call fn5(v14, v3, v6, v8, v10, v12, v13)
    v16 = stack_load.i64 ss0
    v17 = symbol_value.i64 gv1
    v18 = iadd_imm v17, 8
    v19 = iadd_imm v18, 3
    v20 = func_addr.i64 fn7
    v21 = iadd_imm v20, 3
    call fn6(v15, v19, v21)
    v22 = load.i64 v15
    v23 = func_addr.i64 fn10
    v24 = iconst.i64 1
    v25 = iadd_imm v23, 3
    v26 = ishl_imm v1, 1
    v27 = iconst.i64 3
    v28 = call fn11(v27)  ; v27 = 3
    store aligned v25, v28
    store aligned v24, v28+8  ; v24 = 1
    store aligned v26, v28+16
    v29 = iadd_imm v28, 1
    stack_store v16, ss0
    v30 = stack_addr.i64 ss0
    v31 = call fn12(v29, v30)
    v32 = stack_load.i64 ss0
    v33 = isub v16, v32
    v34 = load.i64 v22+8
    v35 = ishl_imm v34, 3
    v36 = iadd v35, v33
    v37 = ushr_imm v36, 3
    store v37, v22+8
    v38 = call fn9(v32, v22, v31, v15)
    v39 = load.i64 v38
    v40 = iadd_imm v38, 8
    v41 = sshr_imm v39, 1
    return v41
}


[interruptAndRecover$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 0
    v4 = call fn0(v3)  ; v3 = 0
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[interruptAndRecover$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 2
    v4 = call fn0(v3)  ; v3 = 2
    store aligned v2, v4
    store aligned v2, v4+8
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[interruptAndRecover$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[interruptAndRecover$__lambda_3__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 0
    v5 = ishl_imm v4, 1  ; v4 = 0
    v6 = iconst.i64 2
    v7 = call fn0(v6)  ; v6 = 2
    store aligned v5, v7
    store aligned v3, v7+8
    v8 = iadd_imm v7, 1
    v9 = iconst.i64 2
    v10 = call fn1(v9)  ; v9 = 2
    store aligned v2, v10
    store aligned v8, v10+8
    v11 = iadd_imm v10, 1
    v12 = iadd_imm v0, 8
    store v11, v12
    v13 = load.i64 v1+8
    v14 = ishl_imm v13, 3
    v15 = iadd v0, v14
    v16 = load.i64 v1
    return_call_indirect sig2, v16(v15, v1, v12)
}


[interruptAndRecover$__lambda_4__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname1
    gv2 = symbol colocated userextname5
    gv3 = symbol colocated userextname1
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig7 = (i64, i64) -> i64 tail
    sig8 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:15 sig0
    fn1 = colocated u0:18 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:15 sig4
    fn4 = colocated u0:18 sig5
    fn5 = u0:4 sig6

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = symbol_value.i64 gv0
    v4 = iadd_imm v3, 8
    v5 = iadd_imm v4, 3
    v6 = global_value.i64 gv1
    v7 = iadd_imm v6, 8
    v8 = iadd_imm v0, -8
    store aligned v2, v8
    v9 = iconst.i64 1
    v10 = func_addr.i64 fn0
    v11 = func_addr.i64 fn1
    v12 = iconst.i64 0
    v13 = call fn2(v5, v8, v7, v9, v10, v11, v12)  ; v9 = 1, v12 = 0
    v14 = load.i64 v13
    v15 = load.i64 v13+8
    v16 = load.i64 v13+16
    v17 = call_indirect sig3, v14(v15, v16)
    v18 = load.i64 v17
    v19 = iadd_imm v17, 8
    v20 = symbol_value.i64 gv2
    v21 = iadd_imm v20, 8
    v22 = iadd_imm v21, 3
    v23 = global_value.i64 gv3
    v24 = iadd_imm v23, 8
    v25 = iconst.i64 31
    v26 = ishl_imm v25, 1  ; v25 = 31
    v27 = iadd_imm v19, -8
    store aligned v26, v27
    v28 = iconst.i64 1
    v29 = func_addr.i64 fn3
    v30 = func_addr.i64 fn4
    v31 = iconst.i64 0
    v32 = call fn5(v22, v27, v24, v28, v29, v30, v31)  ; v28 = 1, v31 = 0
    v33 = load.i64 v32
    v34 = load.i64 v32+8
    v35 = load.i64 v32+16
    v36 = call_indirect sig7, v33(v34, v35)
    v37 = load.i64 v36
    v38 = iadd_imm v36, 8
    v39 = iconst.i64 31
    v40 = ishl_imm v39, 1  ; v39 = 31
    v41 = iadd_imm v0, 0
    store v40, v41
    v42 = load.i64 v1+8
    v43 = ishl_imm v42, 3
    v44 = iadd v0, v43
    v45 = load.i64 v1
    return_call_indirect sig8, v45(v44, v1, v41)
}


[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname7
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64, i64) -> i64 tail
    sig4 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig5 = (i64, i64, i64) apple_aarch64
    sig6 = (i64, i64) -> i64 tail
    sig7 = (i64, i64, i64) apple_aarch64
    sig8 = (i64, i64, i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:33 sig0
    fn1 = colocated u0:35 sig1
    fn2 = colocated u0:37 sig2
    fn3 = colocated u0:20 sig3
    fn4 = u0:6 sig4
    fn5 = u0:7 sig5
    fn6 = colocated u0:39 sig6
    fn7 = u0:8 sig7
    fn8 = u0:13 sig8
    fn9 = colocated u0:41 sig9
    fn10 = u0:1 sig10

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 1
    v4 = ishl_imm v3, 1  ; v3 = 1
    v5 = func_addr.i64 fn0
    v6 = iadd_imm v5, 3
    v7 = func_addr.i64 fn1
    v8 = iadd_imm v7, 3
    v9 = func_addr.i64 fn2
    v10 = iadd_imm v9, 3
    v11 = func_addr.i64 fn3
    stack_store v0, ss0
    v12 = stack_addr.i64 ss0
    v13 = call fn4(v12, v2, v4, v6, v8, v10, v11)
    v14 = stack_load.i64 ss0
    v15 = symbol_value.i64 gv1
    v16 = iadd_imm v15, 8
    v17 = iadd_imm v16, 3
    v18 = func_addr.i64 fn6
    v19 = iadd_imm v18, 3
    call fn5(v13, v17, v19)
    v20 = load.i64 v13
    v21 = func_addr.i64 fn9
    v22 = iadd_imm v21, 3
    stack_store v14, ss0
    v23 = stack_addr.i64 ss0
    v24 = call fn10(v22, v23)
    v25 = stack_load.i64 ss0
    v26 = isub v14, v25
    v27 = load.i64 v20+8
    v28 = ishl_imm v27, 3
    v29 = iadd v28, v26
    v30 = ushr_imm v29, 3
    store v30, v20+8
    v31 = call fn8(v25, v20, v24, v13)
    v32 = load.i64 v31
    v33 = iadd_imm v31, 8
    v34 = sshr_imm v32, 1
    return v34
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 0
    v4 = call fn0(v3)  ; v3 = 0
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 2
    v4 = call fn0(v3)  ; v3 = 2
    store aligned v2, v4
    store aligned v2, v4+8
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iconst.i64 100
    v5 = sshr_imm v2, 1
    v6 = imul v5, v4  ; v4 = 100
    v7 = sshr_imm v3, 1
    v8 = iadd v6, v7
    v9 = ishl_imm v8, 1
    v10 = iadd_imm v0, 8
    store v9, v10
    v11 = load.i64 v1+8
    v12 = ishl_imm v11, 3
    v13 = iadd v0, v12
    v14 = load.i64 v1
    return_call_indirect sig0, v14(v13, v1, v10)
}


[main$__lambda_3__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64) -> i64 apple_aarch64
    sig3 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1
    fn2 = u0:0 sig2

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = sshr_imm v2, 1
    v5 = sshr_imm v3, 1
    v6 = imul v4, v5
    v7 = ishl_imm v6, 1
    v8 = iconst.i64 1
    v9 = ishl_imm v8, 1  ; v8 = 1
    v10 = iconst.i64 0
    v11 = call fn0(v10)  ; v10 = 0
    v12 = iadd_imm v11, 1
    v13 = iconst.i64 2
    v14 = call fn1(v13)  ; v13 = 2
    store aligned v9, v14
    store aligned v12, v14+8
    v15 = iadd_imm v14, 1
    v16 = iconst.i64 2
    v17 = call fn2(v16)  ; v16 = 2
    store aligned v7, v17
    store aligned v15, v17+8
    v18 = iadd_imm v17, 1
    v19 = iadd_imm v0, 8
    store v18, v19
    v20 = load.i64 v1+8
    v21 = ishl_imm v20, 3
    v22 = iadd v0, v21
    v23 = load.i64 v1
    return_call_indirect sig3, v23(v22, v1, v19)
}


[main$__lambda_4__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname1
    gv2 = symbol colocated userextname0
    gv3 = symbol colocated userextname1
    gv4 = symbol colocated userextname0
    gv5 = symbol colocated userextname1
    gv6 = symbol colocated userextname0
    gv7 = symbol colocated userextname1
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64) -> i64 tail
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64) -> i64 tail
    sig7 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 tail
    sig10 = (i64, i64) -> i64 tail
    sig11 = (i64, i64) -> i64 tail
    sig12 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig13 = (i64, i64) -> i64 tail
    sig14 = (i64, i64) -> i64 tail
    sig15 = (i64, i64) -> i64 tail
    sig16 = (i64, i64) -> i64 tail
    sig17 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig18 = (i64, i64) -> i64 tail
    sig19 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:15 sig0
    fn1 = colocated u0:18 sig1
    fn2 = u0:4 sig2
    fn3 = colocated u0:21 sig4
    fn4 = colocated u0:15 sig5
    fn5 = colocated u0:18 sig6
    fn6 = u0:4 sig7
    fn7 = colocated u0:21 sig9
    fn8 = colocated u0:15 sig10
    fn9 = colocated u0:18 sig11
    fn10 = u0:4 sig12
    fn11 = colocated u0:21 sig14
    fn12 = colocated u0:15 sig15
    fn13 = colocated u0:18 sig16
    fn14 = u0:4 sig17

block0(v0: i64, v1: i64):
    v2 = symbol_value.i64 gv0
    v3 = iadd_imm v2, 8
    v4 = iadd_imm v3, 3
    v5 = global_value.i64 gv1
    v6 = iadd_imm v5, 8
    v7 = iconst.i64 2
    v8 = ishl_imm v7, 1  ; v7 = 2
    v9 = iadd_imm v0, -8
    store aligned v8, v9
    v10 = iconst.i64 1
    v11 = func_addr.i64 fn0
    v12 = func_addr.i64 fn1
    v13 = iconst.i64 0
    v14 = call fn2(v4, v9, v6, v10, v11, v12, v13)  ; v10 = 1, v13 = 0
    v15 = load.i64 v14
    v16 = load.i64 v14+8
    v17 = load.i64 v14+16
    v18 = call_indirect sig3, v15(v16, v17)
    v19 = load.i64 v18
    v20 = iadd_imm v18, 8
    v21 = iconst.i64 3
    v22 = call fn3(v20, v21)  ; v21 = 3
    v23 = symbol_value.i64 gv2
    v24 = iadd_imm v23, 8
    v25 = iadd_imm v24, 3
    v26 = global_value.i64 gv3
    v27 = iadd_imm v26, 8
    v28 = ishl_imm v22, 1
    v29 = iadd_imm v20, -8
    store aligned v28, v29
    v30 = iconst.i64 1
    v31 = func_addr.i64 fn4
    v32 = func_addr.i64 fn5
    v33 = iconst.i64 0
    v34 = call fn6(v25, v29, v27, v30, v31, v32, v33)  ; v30 = 1, v33 = 0
    v35 = load.i64 v34
    v36 = load.i64 v34+8
    v37 = load.i64 v34+16
    v38 = call_indirect sig8, v35(v36, v37)
    v39 = load.i64 v38
    v40 = iadd_imm v38, 8
    v41 = iconst.i64 5
    v42 = call fn7(v40, v41)  ; v41 = 5
    v43 = symbol_value.i64 gv4
    v44 = iadd_imm v43, 8
    v45 = iadd_imm v44, 3
    v46 = global_value.i64 gv5
    v47 = iadd_imm v46, 8
    v48 = ishl_imm v42, 1
    v49 = iadd_imm v40, -8
    store aligned v48, v49
    v50 = iconst.i64 1
    v51 = func_addr.i64 fn8
    v52 = func_addr.i64 fn9
    v53 = iconst.i64 0
    v54 = call fn10(v45, v49, v47, v50, v51, v52, v53)  ; v50 = 1, v53 = 0
    v55 = load.i64 v54
    v56 = load.i64 v54+8
    v57 = load.i64 v54+16
    v58 = call_indirect sig13, v55(v56, v57)
    v59 = load.i64 v58
    v60 = iadd_imm v58, 8
    v61 = iconst.i64 7
    v62 = call fn11(v60, v61)  ; v61 = 7
    v63 = symbol_value.i64 gv6
    v64 = iadd_imm v63, 8
    v65 = iadd_imm v64, 3
    v66 = global_value.i64 gv7
    v67 = iadd_imm v66, 8
    v68 = ishl_imm v62, 1
    v69 = iadd_imm v60, -8
    store aligned v68, v69
    v70 = iconst.i64 1
    v71 = func_addr.i64 fn12
    v72 = func_addr.i64 fn13
    v73 = iconst.i64 0
    v74 = call fn14(v65, v69, v67, v70, v71, v72, v73)  ; v70 = 1, v73 = 0
    v75 = load.i64 v74
    v76 = load.i64 v74+8
    v77 = load.i64 v74+16
    v78 = call_indirect sig18, v75(v76, v77)
    v79 = load.i64 v78
    v80 = iadd_imm v78, 8
    v81 = iconst.i64 1
    v82 = ishl_imm v81, 1  ; v81 = 1
    v83 = iadd_imm v0, -8
    store v82, v83
    v84 = load.i64 v1+8
    v85 = ishl_imm v84, 3
    v86 = iadd v0, v85
    v87 = load.i64 v1
    return_call_indirect sig19, v87(v86, v1, v83)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:32 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
