CLIR
========
[main__specialized]
function u0:0(i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname6
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64) -> i64 tail
    sig3 = (i64, i64, i64) -> i64 tail
    sig4 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig5 = (i64, i64) -> i64 tail
    sig6 = (i64, i64, i64, i64) apple_aarch64
    sig7 = (i64, i64, i64, i64) -> i64 tail
    sig8 = (i64, i64) -> i64 tail
    sig9 = (i64, i64) -> i64 apple_aarch64
    fn0 = colocated u0:21 sig0
    fn1 = colocated u0:23 sig1
    fn2 = colocated u0:25 sig2
    fn3 = colocated u0:19 sig3
    fn4 = u0:6 sig4
    fn5 = colocated u0:27 sig5
    fn6 = u0:7 sig6
    fn7 = u0:12 sig7
    fn8 = colocated u0:29 sig8
    fn9 = u0:1 sig9

block0(v0: i64):
    v1 = global_value.i64 gv0
    v2 = iadd_imm v1, 8
    v3 = iconst.i64 2
    v4 = ishl_imm v3, 1  ; v3 = 2
    v5 = func_addr.i64 fn0
    v6 = iadd_imm v5, 3
    v7 = func_addr.i64 fn1
    v8 = iadd_imm v7, 3
    v9 = func_addr.i64 fn2
    v10 = iadd_imm v9, 3
    v11 = func_addr.i64 fn3
    stack_store v0, ss0
    v12 = stack_addr.i64 ss0
    v13 = call fn4(v12, v2, v4, v6, v8, v10, v11)
    v14 = stack_load.i64 ss0
    v15 = symbol_value.i64 gv1
    v16 = iadd_imm v15, 8
    v17 = iadd_imm v16, 3
    v18 = func_addr.i64 fn5
    v19 = iadd_imm v18, 3
    v20 = iconst.i64 2
    call fn6(v13, v17, v19, v20)  ; v20 = 2
    v21 = load.i64 v13
    v22 = func_addr.i64 fn8
    v23 = iadd_imm v22, 3
    stack_store v14, ss0
    v24 = stack_addr.i64 ss0
    v25 = call fn9(v23, v24)
    v26 = stack_load.i64 ss0
    v27 = isub v14, v26
    v28 = load.i64 v21+8
    v29 = ishl_imm v28, 3
    v30 = iadd v29, v27
    v31 = ushr_imm v30, 3
    store v31, v21+8
    v32 = call fn7(v26, v21, v25, v13)
    v33 = load.i64 v32
    v34 = iadd_imm v32, 8
    v35 = sshr_imm v33, 1
    return v35
}


[main$__lambda_0__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 0
    v4 = call fn0(v3)  ; v3 = 0
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[main$__lambda_1__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 2
    v4 = call fn0(v3)  ; v3 = 2
    store aligned v2, v4
    store aligned v2, v4+8
    v5 = iadd_imm v4, 1
    v6 = iadd_imm v0, 0
    store v5, v6
    v7 = load.i64 v1+8
    v8 = ishl_imm v7, 3
    v9 = iadd v0, v8
    v10 = load.i64 v1
    return_call_indirect sig1, v10(v9, v1, v6)
}


[main$__lambda_2__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64, i64, i64) -> i64 tail

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = load.i64 v0+8
    v4 = iadd_imm v0, 8
    store v3, v4
    v5 = load.i64 v1+8
    v6 = ishl_imm v5, 3
    v7 = iadd v0, v6
    v8 = load.i64 v1
    return_call_indirect sig0, v8(v7, v1, v4)
}


[main$__lambda_3__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    sig0 = (i64) -> i64 apple_aarch64
    sig1 = (i64) -> i64 apple_aarch64
    sig2 = (i64, i64, i64) -> i64 tail
    fn0 = u0:0 sig0
    fn1 = u0:0 sig1

block0(v0: i64, v1: i64):
    v2 = load.i64 v0
    v3 = iconst.i64 0
    v4 = ishl_imm v3, 1  ; v3 = 0
    v5 = iconst.i64 5
    v6 = ishl_imm v5, 1  ; v5 = 5
    v7 = iconst.i64 2
    v8 = call fn0(v7)  ; v7 = 2
    store aligned v4, v8
    store aligned v6, v8+8
    v9 = iadd_imm v8, 1
    v10 = iconst.i64 2
    v11 = call fn1(v10)  ; v10 = 2
    store aligned v2, v11
    store aligned v9, v11+8
    v12 = iadd_imm v11, 1
    v13 = iadd_imm v0, 0
    store v12, v13
    v14 = load.i64 v1+8
    v15 = ishl_imm v14, 3
    v16 = iadd v0, v15
    v17 = load.i64 v1
    return_call_indirect sig2, v17(v16, v1, v13)
}


[main$__lambda_4__cps]
function u0:0(i64, i64) -> i64 tail {
    ss0 = explicit_slot 8
    gv0 = symbol colocated userextname0
    gv1 = symbol colocated userextname1
    sig0 = (i64, i64) -> i64 tail
    sig1 = (i64, i64) -> i64 tail
    sig2 = (i64, i64, i64, i64, i64, i64, i64) -> i64 apple_aarch64
    sig3 = (i64, i64) -> i64 tail
    sig4 = (i64, i64, i64) -> i64 tail
    fn0 = colocated u0:14 sig0
    fn1 = colocated u0:17 sig1
    fn2 = u0:4 sig2

block0(v0: i64, v1: i64):
    v2 = symbol_value.i64 gv0
    v3 = iadd_imm v2, 8
    v4 = iadd_imm v3, 3
    v5 = global_value.i64 gv1
    v6 = iadd_imm v5, 8
    v7 = iconst.i64 0
    v8 = func_addr.i64 fn0
    v9 = func_addr.i64 fn1
    v10 = iconst.i64 0
    v11 = call fn2(v4, v0, v6, v7, v8, v9, v10)  ; v7 = 0, v10 = 0
    v12 = load.i64 v11
    v13 = load.i64 v11+8
    v14 = load.i64 v11+16
    v15 = call_indirect sig3, v12(v13, v14)
    v16 = load.i64 v15
    v17 = iadd_imm v15, 8
    v18 = iconst.i64 1
    v19 = ishl_imm v18, 1  ; v18 = 1
    v20 = iadd_imm v0, -8
    store v19, v20
    v21 = load.i64 v1+8
    v22 = ishl_imm v21, 3
    v23 = iadd v0, v22
    v24 = load.i64 v1
    return_call_indirect sig4, v24(v23, v1, v20)
}


[__main__]
function u0:0() -> i64 fast {
    sig0 = () -> i64 apple_aarch64
    sig1 = (i64) -> i64 tail
    fn0 = u0:2 sig0
    fn1 = colocated u0:20 sig1

block0:
    v0 = call fn0()
    v1 = call fn1(v0)
    return v1
}
